<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-机器学习第四章" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/08/机器学习第四章/" class="article-date">
  <time datetime="2018-07-08T07:58:30.000Z" itemprop="datePublished">2018-07-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/08/机器学习第四章/">机器学习第四章</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="4-1-分类问题"><a href="#4-1-分类问题" class="headerlink" title="4.1 分类问题"></a>4.1 分类问题</h2><p>在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈等等。<br>我们从二元的分类问题开始讨论。<br>我们将因变量（dependant variable）可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量$$ y∈{0,1} $$，其中0表示负向类，1表示正向类。</p>
<h2 id="4-2-分类问题建模"><a href="#4-2-分类问题建模" class="headerlink" title="4.2 分类问题建模"></a>4.2 分类问题建模</h2><p>回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线：<br><img src="https://iajqs.github.io/img/oneLine.jpg" alt=""><br>根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测：（我想这里有个值得注意的点，线性回归是解决回归问题，逻辑回归是解决分类问题，至于逻辑回归这个名字的命名原因， 是由于其中使用了一个sigmoid函数作为中间结果，而这个中间结果就是一个连续的值。）<br>-当h_θ大于等0.5时，预测y=1<br>-当h_θ小于0.5时，预测y=0<br>对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。假使我们又预测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。<br><img src="https://iajqs.github.io/img/oneLine2.png" alt=""><br>这时，再使用0.5作为阈值来预测肿瘤是良性还是恶性便不合适了。可以看出，线性回归模型，因为其预测的值可以超过[0,1]的范围，并不适合解决这样的问题。<br>我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0到1之间。<br>逻辑回归模型的假设是：$$ h_θ(X) = g(θ^TX) $$<br>其中：<br>-X代表特征向量<br>-g代表逻辑函数（logistic function）是一个常用的逻辑函数为s形函数（Sigmoid function），公式为$$ g(z) = {1 \over 1+e^{-z}} $$<br>该函数的图像为：<br><img src="https://iajqs.github.io/img/sigmoid.jpg" alt=""><br>合起来，我们得到逻辑回归模型的假设：<br>$$<br>h_θ(x) = {1 \over 1+e^{-θ^TX}}<br>$$对于给定的输入<br>对模型的理解：<br>$$ h_θ(x) $$的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性（estimated probability）即 $$ h_θ(x)=P(y=1|x;θ) $$<br>例如，如果对于给定的x，通过已经确定的参数计算得出$$ h_θ(x) = 0.7 $$，则表示有百分之70的几率y为正向类，相应地y为负向类的几率为<code>1-0.7=0.3.</code></p>
<h2 id="5-3判定边界"><a href="#5-3判定边界" class="headerlink" title="5.3判定边界"></a>5.3判定边界</h2><p>在逻辑回归中，我们预测：<br>-当h_θ大于等于0.5时，预测y=1<br>-当h_θ小于0.5时，预测y=0<br>根据上面绘制出的S形函数图像，我们知道当<br>-z=0时g(z)=0.5<br>-z&gt;0时g(z)&gt;0.5<br>-z&lt;0时g(z)&lt;0.5<br>又z=θ^TX，即<br>-θ^T大于等于0时，预测y=1<br>-θ^T小于0时，预测y=0<br>现在假设我们有一个模型：$$ h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2) $$并且参数θ是向量[-3 1 1]。<br>则当<code>-3+x_1+x_2</code>大于等于0，即<code>x_1+x_2</code>大于等于3时，模型将预测y=1。<br>我们可以绘制直线<code>x_1+x_2=3</code>，这条线便是我们模型的分界线，将预测为1的区域和预测为0的区域分隔开。<br><img src="https://iajqs.github.io/img/oneLine3.jpg" alt=""><br>即使我们的数据呈现这样的分布情况，怎样的模型才能合适呢？<br><img src="https://iajqs.github.io/img/oneLine4.jpg" alt=""><br>因为需要用曲线才能分隔y=0的区域和y=1的区域，我们需要二次方特征：<br>$$ h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2+θ_3x_1^2+θ_4x_2^2) $$<br>假设参数是[-1 0 0 1 1]，则我们得到的判定边界恰好是圆点在原点且半径为1的圆形。<br>我们可以用哪个非常复杂的模型来适应非常复杂形状的判定边界。</p>
<h2 id="5-4-代价函数"><a href="#5-4-代价函数" class="headerlink" title="5.4 代价函数"></a>5.4 代价函数</h2><p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将$$ h_θ(X)={1 \over 1+e^{-θ^TX}} $$带入到这样定义了的代价函数中，我们得到的代价函数将是一个非凸函数(non-convex function)。<br><img src="https://iajqs.github.io/img/costFunction4.jpg" alt=""><br>这意味着我们的代价函数有很多局部最小值，这将影响梯度下降法寻找全局最小值。因此我们重新定义逻辑回归的代价函数为：<br><img src="https://iajqs.github.io/img/costFunction5.png" alt=""><br>其中<br><img src="https://iajqs.github.io/img/cost.png" alt=""><br>$$ h_θ(x) $$ 与 $$ Cost(h_θ(x),y) $$之间的关系如下图所示：<br><img src="https://iajqs.github.io/img/costFunction6.jpg" alt=""><br>这样构建的$$ Cost(h_θ(x),y) $$函数的特点是：当实际的y=1且$$ h_θ $$也为1时误差为0，当y=1但$$ h_θ $$不为1时误差随着$$ h_θ $$的变小而变大；当实际的y=0且$$h_θ$$也为0时代价为0，当y=0但$$ h_θ $$不为0时的误差随着$$ h_θ $$的变大而变大。<br>将构建的$$ Cost(h_θ(x),y) $$简化如下：<br><img src="https://iajqs.github.io/img/costFunction7.png" alt=""><br>带入代价函数得到：<br><img src="https://iajqs.github.io/img/costFunction8.png" alt=""><br>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：<br><img src="https://iajqs.github.io/img/GradientDescentforMultiple1.png" alt=""><br><br>求导后得到：<br><img src="https://iajqs.github.io/img/GradientDescentforMultiple3.png" alt=""><br><br>注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降法一样，但是这里的$$ h_θ(x)=g(θ^TX)$$与线性回归中不同，所以实际上是不一样的，另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。<br>一些梯度下降算法之外的选择：<br>除了梯度下降算法以外还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速，这些算法有：共轭梯度（Conjugate Gradient），局部优化法（Broyden fletcher goldfarb shann， BFGS）和有限内存局部优化法（LBFGS）<br>fminunc 是 matlab 和 octave 中都带的一个最小值优化函数，使用时我们需要提供代价函数<br>和每个参数的求导，下面是 octave 中使用 fminunc 函数的代码示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function [jVal, gradient] = costFunction(theta)</span><br><span class="line">jVal = [...code to compute J(theta)...];</span><br><span class="line">gradient = [...code to compute derivative of J(theta)...];</span><br><span class="line">end</span><br><span class="line">options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, &apos;100&apos;);</span><br><span class="line">initialTheta = zeros(2,1);</span><br><span class="line">[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure></p>
<h2 id="5-5-多类分类"><a href="#5-5-多类分类" class="headerlink" title="5.5 多类分类"></a>5.5 多类分类</h2><p>多类分类问题中，我们的训练集中有多个类（&gt;2），我们无法仅仅用一个二元变量（0或1）来做判断依据，例如我们要预测天气情况分四中类型：晴天，多云，下雨或下雪。<br>下面是一个多分类问题可能的情况。<br><img src="https://iajqs.github.io/img/multiclass_classification.png" alt=""><br>一种解决这类问题的途径是采用一对多（One-vs-All）方法。在一对多方法中，我们将多类分类问题转化成二元分类问题。为了能实现这样的转变，我们将多个类中的一个类标记为正向类(y=1)，然后将其他所有类都标记为负向类，这个模型记作$$ h_θ^{(1)}(x) $$。接着，类似地我们选择另一类类标记为正向类（y=2），再将其他类都标记为负向类，将这个模型记作$$ h_θ^{(2)}(x) $$，以此类推。<br>最后我们得到一系列的模型标记为$$ h-θ^{i}（x）=p(y=i|x;θ) $$ 其中i=(1,2,3,…,k)<br><img src="https://iajqs.github.io/img/multiclass_classification2.png" alt=""><br>最后，在我们需要做预测时，我们将所有的分类机制都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>黄海广博士的笔记：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md</a><br>小小人_V 的个人笔记 ​ <a href="https://mooc.guokr.com/note/12/" target="_blank" rel="noopener">https://mooc.guokr.com/note/12/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/08/机器学习第四章/" data-id="cjjcsffgz000fccnwsczn8hp0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-机器学习第三章" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/06/机器学习第三章/" class="article-date">
  <time datetime="2018-07-06T13:26:32.000Z" itemprop="datePublished">2018-07-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/06/机器学习第三章/">机器学习第三章</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h1><h2 id="3-1-多维特征"><a href="#3-1-多维特征" class="headerlink" title="3.1 多维特征"></a>3.1 多维特征</h2><p>目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间楼层数等，构成一个含有多个变量的模型，模型中的特征为($$ x_1,x_2,…,x_n $$)<br><img src="https://iajqs.github.io/img/multiFeatures.png" alt=""><br><br>增添更多特征后，我们引入一系列新的注释：<br>    n 代表特征的数量<br>    $$ x^(i) $$ 代表第i个训练实例，是特征矩阵中的第i行，是一个向量(vector)<br>    $$ x_j^(i) $$ 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征。<br>支持多变量的假设h表示为：<br>$$ h_θ…(x)=θ_0+θ_1x_1+θ_2x_2+…+θ_nx_n $$<br>这个公式中有n+1个参数和n个变量，为了使得公式能够简化一些，引入$$ x_0 = 1 $$,则公式转化为：<br>$$ h_θ…(x)=θ_0x_0+θ_1x_1+θ_2x_2+…+θ_nx_n $$<br>此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是$ m*n+1 $。<br>因此公式可以简化为<br>$$ h_θ^(x)=θ^TX $$<br>其中上标T代表矩阵转置。</p>
<h2 id="3-2-多变量梯度下降"><a href="#3-2-多变量梯度下降" class="headerlink" title="3.2 多变量梯度下降"></a>3.2 多变量梯度下降</h2><p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：<br>$$<br>J(θ_0,θ_1,…,θ<em>n) = {1 \over 2m}\sum</em>(i=1)^m{(h_θ(x^(i))-y^(i))^2}<br>$$<br>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。<br>多变量线性回归的批量梯度下降算法为：<br><img src="https://iajqs.github.io/img/GradientDescentforMultiple1.png" alt=""><br><br>即<br><img src="https://iajqs.github.io/img/GradientDescentforMultiple2.png" alt=""><br><br>求导数后得到：<br><img src="https://iajqs.github.io/img/GradientDescentforMultiple3.png" alt=""><br><br>我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环知道收敛。</p>
<h2 id="3-3-特征缩放"><a href="#3-3-特征缩放" class="headerlink" title="3.3 特征缩放"></a>3.3 特征缩放</h2><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。<br>以放假问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。<br><img src="https://iajqs.github.io/img/FeatureScaling.jpg" alt=""><br><br>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。简单的方法是令：<br>$$<br>x_n={x_n-μ_n \over x_n}<br>$$<br>其中$$ μ_n $$是平均值，$$ s_n $$标准值。</p>
<h2 id="3-4-学习率"><a href="#3-4-学习率" class="headerlink" title="3.4 学习率"></a>3.4 学习率</h2><p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时区域收敛。<br><img src="https://iajqs.github.io/img/LearningRate.jpg" alt=""><br><br>也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阈值(例如0.001)进行比较，但通常看上面这样的图表更好。<br>梯度下降法算法的每次迭代受到学习率的影响，如果学习率α过小，则达到收敛所需的迭代次数会非常高；如果学习率α过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。<br>通常可以考虑尝试这些学习率：<br>$α=0.01,0.3,0.1,0.3,1,3,10$ </p>
<h2 id="3-5-多项式回归"><a href="#3-5-多项式回归" class="headerlink" title="3.5 多项式回归"></a>3.5 多项式回归</h2><p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型： $$ h_θ(x)=θ_0+θ_1x_1+θ_2x_2^2 $$ 或者 $$ h_θ(x)=θ_0+θ_1x_1+θ_2x_2^2+θ_3x_3^3 $$<br><img src="https://iajqs.github.io/img/polynomialRegression.jpg" alt=""><br><br>通常我们需要先观察数据然后再决定准备尝试怎样的模型。<br>另外，我们可以令：<br>$$ x_2=x_2^2 $$<br>$$ x_3=x_3^3 $$<br>从而将模型转化为线性回归模型。注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</p>
<h2 id="3-6-正规方程"><a href="#3-6-正规方程" class="headerlink" title="3.6 正规方程"></a>3.6 正规方程</h2><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。<br>正规方程式通过求解下面的方程来找出使得代价函数最小的参数的：$$ {\partial \over \partial θ_j}J(θ_j)=0 $$<br>假设我们的训练集特征矩阵为X(包含了$$ x_0=1 $$)并且我们的训练集结果为向量y，则利用正规方程解出向量$$ θ={X^T×X}^{-1}×X^T×y $$<br>上标T代表矩阵转置，上标-1代表矩阵的逆。<br>以下表所示数据为例：<br>examples.png<br><img src="https://iajqs.github.io/img/examples.png" alt=""><br><br>运用正则方程方法求解参数：<br><img src="https://iajqs.github.io/img/caculate.jpg" alt=""><br><br>在Octave中，正规方程写作：<br><code>pinv(X&#39;*X)*X&#39;*y</code><br>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。<br>梯度下降与正规方程的比较：<br>梯度下降 | 正规方程 </p>
<ul>
<li>| -:<br>需要选择学习率α | 不需要<br>需要多次迭代 | 一次运算得出<br>当特征数量 n 大时也能较好适用 | 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$$ O(n^3) $$通常来说当n小于10000时还是可以接受的<br>适用于各种类型的模型 | 只适用于线性模型，不适合逻辑回归模型等其他模型 </li>
</ul>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>黄海广博士的笔记：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md</a><br>小小人_V 的个人笔记 ​ <a href="https://mooc.guokr.com/note/12/" target="_blank" rel="noopener">https://mooc.guokr.com/note/12/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/06/机器学习第三章/" data-id="cjjcsffgq000bccnwtf87wz9x" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-机器学习第二章" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/06/机器学习第二章/" class="article-date">
  <time datetime="2018-07-06T03:37:45.000Z" itemprop="datePublished">2018-07-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/06/机器学习第二章/">机器学习第二章</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="2-单变量线性回归"><a href="#2-单变量线性回归" class="headerlink" title="2 单变量线性回归"></a>2 单变量线性回归</h1><h2 id="2-1-模型表达"><a href="#2-1-模型表达" class="headerlink" title="2.1 模型表达"></a>2.1 模型表达</h2><p>以之前的房屋交易问题为例，假使我们回归问题的训练集（Training Set）如下表所示：<br><img src="https://iajqs.github.io/img/trainSet.png" alt=""><br><br>我们将要用来描述这个回归问题的标记如下：<br>· m 代表训练集中示例的数量<br>· x 代表特征/输入变量<br>· y 代表目标变量/输出变量<br>· (x, y) 代表训练集中的实例<br>· (x^1,y^1) 代表第i个观察实例<br>· h 代表学习算法的解决方案或函数也称为假设(hypothesis)<br><img src="https://iajqs.github.io/img/hypothesis.png" alt=""><br><br>因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的算法，进而学习得一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。<br>那么，对于我们的房价预测问题，我们该如何表达h？<br>一种可能的表达方式为：<code>h_theta = theta0 + theta1 * x</code>(由于现在没有网，也不知道怎么打出theta，暂时用英文代替吧)<br>因为只含有一个特征/输入变量，因此这样的问题叫做单变量线性回归问题。</p>
<h2 id="2-2-代价函数"><a href="#2-2-代价函数" class="headerlink" title="2.2 代价函数"></a>2.2 代价函数</h2><p>我们现在要做的便是为我们的模型选择合适的参数（parameters）theta0 和 theta1，在房价问题这个例子中便是直线的斜率和在y轴上的截距。<br>我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值域训练集中实际值之间的差距（下图中蓝线所指）就是模型误差（modeling error）。<br><img src="https://iajqs.github.io/img/modelingError.png" alt=""><br><br>我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。<br>即使的代价函数<br><img src="https://iajqs.github.io/img/costFunction1.png" alt=""><br><br>最小。<br>我们绘制一个等高线图，三个坐标分别为 theta0 和 theta1 和 J(theta0, theta1)：<br><img src="https://iajqs.github.io/img/3D_costFunction.png" alt=""><br><br>以上是代价函数的样子，在等高线图中，则可以看出在三维空间中存在一个使得J(theta0，theta1)最小的点。<br><img src="https://iajqs.github.io/img/equalLine.png" alt=""><br></p>
<h2 id="2-3-代价函数的直观理解I"><a href="#2-3-代价函数的直观理解I" class="headerlink" title="2.3 代价函数的直观理解I"></a>2.3 代价函数的直观理解I</h2><p>我们给了代价函数一个数学上的定义。在里，让我们通过一些例子来获取一些直观的感受，看看代价函数到底是在干什么。<br><img src="https://iajqs.github.io/img/costFunction2.png" alt=""><br><br><img src="https://iajqs.github.io/img/costFunction3.png" alt=""><br></p>
<h2 id="2-4-梯度下降"><a href="#2-4-梯度下降" class="headerlink" title="2.4 梯度下降"></a>2.4 梯度下降</h2><p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数 J(theata0, theta1) 的最小值。<br>梯度下降背后的思想是：开始时我们随机选择一个参数的组合(theta0, theta1,…,thetan)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到一个局部最小值(local minimum)，因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值(global minimum)，选择不同的初始参数组合，可能会找到不同的局部最小值。<br><img src="https://iajqs.github.io/img/gradientDescent.png" alt=""><br><br>梯度下降算法如下：<br><img src="https://iajqs.github.io/img/batchGradientDescent1.png" alt=""><br><br>描述：对 theta 赋值，使得J(theta)按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。<br><img src="https://iajqs.github.io/img/gradientDescent2.png" alt=""><br><br>批量梯度下降(batch gradient descent)算法的公式为：<br><img src="https://iajqs.github.io/img/batchGradientDescent2.png" alt=""><br><br>其中α 是学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的方向<br>向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。</p>
<h2 id="2-4-对线性回归运用梯度下降法"><a href="#2-4-对线性回归运用梯度下降法" class="headerlink" title="2.4 对线性回归运用梯度下降法"></a>2.4 对线性回归运用梯度下降法</h2><p>对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：<br><img src="https://iajqs.github.io/img/derivativeCostFunction.png" alt=""><br><br><img src="https://iajqs.github.io/img/derivativeCostFunction2.png" alt=""><br><br>则算法改写成：<br><img src="https://iajqs.github.io/img/derivativeCostFunction3.png" alt=""><br></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>黄海广博士的笔记：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md</a><br>小小人_V 的个人笔记 ​ <a href="https://mooc.guokr.com/note/12/" target="_blank" rel="noopener">https://mooc.guokr.com/note/12/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/06/机器学习第二章/" data-id="cjjcsffgt000dccnwqzgw5ae3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-机器学习第一章" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/05/机器学习第一章/" class="article-date">
  <time datetime="2018-07-05T04:20:03.000Z" itemprop="datePublished">2018-07-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/05/机器学习第一章/">机器学习第一章</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>由于我是还算是个学业不精的人吧，写总结的能力也不是很强，所以准备直接抄写黄海广博士的笔记和自己看吴恩达老师视频时的一些总结来形成自己的博客，与其说是博客或者笔记什么的，更应该说是一种抄录吧，纯粹是为了加深个人理解和记忆。</p>
<h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><h1 id="TOC-引言（Introduction）"><a href="#TOC-引言（Introduction）" class="headerlink" title="[TOC]引言（Introduction）"></a>[TOC]引言（Introduction）</h1><h2 id="1-1-欢迎"><a href="#1-1-欢迎" class="headerlink" title="1.1 欢迎"></a>1.1 欢迎</h2><p>参考视频： 1-1 - Welcome.mkv<br>第一个视频主要就讲了什么是机器学习，机器学习能做些什么事情。<br>机器学习是目前信息技术中最激动人心的方向之一。在这门课中，你将学习到这门技术的前沿，并可以自己实现学习机器学习的算法。<br>你或许每天都在不知不觉中使用了机器学习的算法，每次你打开谷歌，必应搜索到你需要的内容，正是因为他们有良好的学习算法。谷歌和微软实现了学习算法来排行网页，每次你用Facebook或苹果的图片分类程序，他能认出你朋友的照片，这也是机器学习。每次您阅读您的电子邮件垃圾邮件筛选器，可以帮你过滤大量的垃圾邮件，这也是一种学习算法。对我来说，我感到激动的原因之一是有一天做出一个和人类一样聪明的机器。实现这个想法任重而道远，许多AI研究者认为，实现这个目标的最好方法是<br>通过让机器试着模仿人的大脑学习，我会在这门课中介绍一点这方面的内容。<br>在这门课中，还讲到学习到关于机器学习的前沿状况。但事实上只了解算法、数学并不能解决你关心的实际问题。所以，我们将花大量的时间做练习，从而你自己能实现每个这些算法，从而了解内部机理。<br>那么，为什么机器学习如此受欢迎呢？原因是，机器学习不只是用于人工智能领域。<br>我们创建智能的机器，有很多基础的知识。比如，我们可以让机器找到A与B之间的最短路径，但我们仍然不知道怎么让机器做更有趣的事情，如web搜索、照片标记、反垃圾邮件。我们发现，唯一方法是让机器自己学习怎么来解决问题。所以，机器学习已经成为计算机的一个能力。<br>现在它涉及到各个行业和基础科学中。我从事于机器学习，但我每个星期都跟直升机飞行员、生物学家、很多计算机系统程序员交流（我在斯坦福大学的同事同时也是这样）和平均每个星期会从硅谷收到两、三个电子邮件，这些联系我的人都对将学习算法应用于他们自己的问题感兴趣。这表明机器学习涉及的我问题非常广泛。有机器人、计算生物学、硅谷中大量的问题都收到机器学习的影响。<br>这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集，比如说，大量的硅谷公司正在收集web上的单击数据，也成为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并未用户提供更好的服务。这在硅谷有巨大的时长。再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。再比如，工程方面，在工程的所有领域，我们有越来越大的数据集我们试图使用学习算法，来理解这些数据，另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。<br>手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你写一个像这样的信封，一种学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。<br>事实上，如果你看到自然语言处理或计算机视觉，这些语言理解或图像理解都是属于AI领域。大部分的自然语言处理和大部分的计算机视觉，都应用了机器学习。学习算法还广泛用于自定制程序。每次你去亚马逊或NetFlix或iTunes Genius，它都会给出其他电影或产品或音乐的建议，这是一种学习算法。仔细想一想，他们有百万的用户；但他们没有办法为百万用户，编写百万个不同程序。软件能给这些自定制的建议的唯一方法是通过学习你的行为，来为你定制服务。<br>最后学习算法被用来理解人类的学习和了解大脑。<br>我们将讨论如何用这些推进我们的AI梦想。几个月前，一名学生给我一片文章关于最顶尖的12个IT技能。拥有了这些技能HR绝对不会拒绝你。这是稍显陈旧的文章，但在这个列表最顶部就是机器学习的技能。<br>在斯坦福大学，招聘人员联系我，让我推荐机器学习学生毕业的人远远多余机器学习的毕业生。所以我认为需求远远没有满足现在“机器学习”非常好，在这门课中，我希望能告诉你们很多机器学习的知识。<br>在接下来的视频中，我们将开始给更正式的定义，什么是机器学习。我们会开始学习机器学习的主要问题和算法，你会了解一些主要的机器学习的术语，并开始了解不同的算法，用哪些算法更合适。</p>
<h2 id="1-2-机器学习是什么？"><a href="#1-2-机器学习是什么？" class="headerlink" title="1.2 机器学习是什么？"></a>1.2 机器学习是什么？</h2><p>参考视频： 1-2 - What is Machine Learning<br>机器学习是什么？在本视频中，我们会尝试着进行定义，同时让你懂得何时会使用机器学习。实际上，即使是在机器学习的专业人士中，也不存在一个被广泛认可的定义来准确定义机器学习是什么活不是什么，现在我将告诉你一些人们尝试定义的示例。第一个机器学习的定义来自于Arthur Samuel，他定义机器学习为，在进行特定编程的情况下，给予计算机学习能力的领域。Samuel的定义可以回溯到50年代，他编写了一个西洋跳棋程序。这程序神奇之处在于，编程者自己并不是下棋高手。但因为他太菜了，于是就通过编程，让西洋棋程序自己跟自己下了上万盘棋。通过观察哪种布局（棋盘位置）会赢，哪种布局会输，久而久之，这西洋棋程序明白了什么是好的布局，什么样式坏的布局。然后就牛逼大发了，程序通过学习后，玩西洋棋的水平超过了Samuel。这绝对是令人注目的成果。<br>尽管编写者自己是个菜鸟，但因为计算机有着足够的耐心，去下上万盘的棋，没有人有这种耐心下这么多盘棋。通过这些学习，计算机获得无比丰富的经验，于是渐渐称为了比Samuel更厉害的西洋棋手。上述是个有点不正式的定义，也比较古老，另一个年代近一点的定义，由Tom Mitchell提出，来自卡内基梅隆大学，Tom定义的机器学习是一个好的学习问题，定义如下，他说，一个程序被认定为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。我认为经验E就是程序上万次的自我练习的经验而任务T就是下棋。性能度量值P呢，就是它在与一些新的对手比赛时，赢得比赛的概率。<br>以一些机器学习的应用举例：</p>
<pre><code>1. 数据库挖掘
2. 一些无法通过手动编程来编写的应用：如自然语言处理，计算机视觉
3. 一些自助式的程序：如推荐系统
4. 理解人类是如何学习的。
</code></pre><h2 id="1-3-监督学习"><a href="#1-3-监督学习" class="headerlink" title="1.3 监督学习"></a>1.3 监督学习</h2><p>在课程稍后阶段我们再给监督学习一个更加正式的定义，现在我们从一个例子开始：<br>假设你有下面这些房价数据，图标上的每个实例都是一次房屋交易，横坐标为交易房屋的占地面积，纵坐标为房屋的交易价格。<br><img src="https://iajqs.github.io/img/Housing-price.png" alt=""><br><br>现在，假设你希望能够预测一个750平方英尺的房屋的交易价格可能是多少。一种方法是根据这些数据点的分布，画一条合适的直线，然后根据这条直线来预测。在房价预测这个例子中，一个二次函数可能更适合已有的数据，我们可能会更希望用这个二次函数的曲线来进行预测。<br><img src="https://iajqs.github.io/img/Housing-price2.PNG" alt=""><br><br>我们称这样的学习为监督学习。称其为监督式，因为我们预先给了算法“正确结果”–即所有我们观察到的变量。<br>上面这个问题又称为回归问题（Regression），因为我们能预测的结果是连续地值。<br>再来看另一种类型的监督学习问题：<br>假使你希望约测一个乳腺癌是否是恶性的，你现在有的数据时不同年龄的病人和她们身上肿瘤的尺寸以及这些肿瘤是否是恶性的。如果我们将这些信息绘制成一张2D图表，以横坐标为肿瘤的尺寸，以纵坐标为病人的年龄，以O代表良性肿瘤，以X代表恶性肿瘤。则我们的算法要学习的问题就变成了如何分割良性肿瘤和恶性肿瘤。<br><img src="https://iajqs.github.io/img/Tumor.PNG" alt=""><br><br>这样的问题是分类问题（Classification），我们希望算法能够学会如何将数据分类到不同的类里。<br>上面的例子中我们只使用了两个特征（feautres）来进行分类，现实中，我们会有非常多的特征，并且我们希望算法能够处理无限多数量的特征，在课程后面我们会介绍能够处理这样问题的算法，例如支持向量机（Support Vector Machine）。</p>
<h2 id="1-4-非监督学习"><a href="#1-4-非监督学习" class="headerlink" title="1.4 非监督学习"></a>1.4 非监督学习</h2><p>在监督学习中，无论是回归问题还是分类问题，我们的数据都具有一个结果（房价问题中的房价，肿瘤问题中的良性与否）。<br>而在非监督学习中，我们的现有数据中并没有结果，我们有的只是特征，因而非监督学习要解决的问题是发现这些数据是否可以分为不同的组。<br><img src="https://iajqs.github.io/img/unsupervise.PNG" alt=""><br><br>非监督学习的一个例子是聚类问题（Clustering），例如对一个大型的数据中心的网络传输数据情况进行分析，发现那些多数时候是在协作的计算机。<br>再一个例子，给定一些人和他们所有的基因，非监督学习可以根据是否具有某些基因而将这些人聚类：<br><img src="https://iajqs.github.io/img/genes.PNG" alt=""><br><br>图中纵坐标为一个个以及他们的基因，横坐标为各种类型的基因。<br>再一个非监督学习问题的例子是鸡尾酒会问题（Cocktail Party Problem），在一个满是人的房间中，人们都在互相对话，我们使用一些麦克风录下房间中的声音，利用非监督学习算法来识别房间中某一个人所说的话。<br>鸡尾酒会问题的一个简化版本是一个房间中有两个人同时在讲话，利用两个麦克风录音。<br><img src="https://iajqs.github.io/img/cocktail.PNG" alt=""><br><br>下面这个只有一行的机器学习算法（Octave）可以非常漂亮地将两个人的说话给分离开来：<br>[W,s,v] = svd((repmat(sum(x.<em>x,1),size(x,1),1).</em>x)*x’);</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>黄海广博士的笔记：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md</a><br>小小人_V 的个人笔记 ​ <a href="https://mooc.guokr.com/note/12/" target="_blank" rel="noopener">https://mooc.guokr.com/note/12/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/05/机器学习第一章/" data-id="cjjcsffgw000eccnw3sivuaqf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-rnn" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/14/rnn/" class="article-date">
  <time datetime="2018-06-14T01:26:04.000Z" itemprop="datePublished">2018-06-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/14/rnn/">rnn</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <pre><code>Word2vec理解得比较清晰了，虽然关于CBOW中使用的哈夫曼树的实际作用和操作并没有完全理解清楚，但我感觉那不是我现在应该纠结的东西，我现在的目标是先把已经学了的东西掌握起来，至于这些更加深层的模型优化算法，以后再说了，所以我现在之类存个记录，为word2vec加入一个待完善的标记。
这一篇将讨论RNN，关于RNN的模型和一些应用。
</code></pre><h1 id="1-RNN概述"><a href="#1-RNN概述" class="headerlink" title="1.RNN概述"></a>1.RNN概述</h1><pre><code>像CNN，训练样本的输入和输出都是比较确定的。CNN能自行提取特征，性能非常优秀，但是如果训练样本是连续的序列，且序列的长短不一，比如基于时间的序列，一段文字等等，我们将主要讨论文字层面的内容。
下面我们来看RNN的模型。
</code></pre><h1 id="2-RNN模型"><a href="#2-RNN模型" class="headerlink" title="2.RNN模型"></a>2.RNN模型</h1><pre><code>这里我直接照搬深度学习（圣经）里的内容。
循环神经网络中一些重要的设计模式包括一下几种：
（1）每个时间步都有输出，并且隐藏单元之间有循环链接的循环网络，如图1所示。
（2）每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环链接的循环网络，如图2所示。（不懂）
（3）隐藏单元之间存在会循环链接，但读取整个序列后产生单个输出的循环网络，如图3所示。（不懂）
</code></pre><p>ps：我的天，除了第一种，其他的都不知道怎么回事。<br><img src="https://iajqs.github.io/img/RNN1.png" alt=""><br><br>    上图中左边是RNN模型没有按时间展开的图，如果按时间序列展开，则是上图中的右边部分。我们重点观察右边部分的图。<br>    这幅图描述了在序列索引号$t$附近RNN的模型。其中：<br>    1）$$x^{(t)}$$代表在序列索引号$t$时训练样本的输入。同样的，$x^{(t-1)}$和$x^{(t+1)}$代表在序列索引号$t-1$和$t+1$时训练样本的输入。<br>    2）$h^{(t)}$代表在序列索引号$t$时模型的隐藏状态。$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$共同决定。<br>    3）$o^{(t)}$代表在序列索引号$t$时模型的输出。$o^{(t)}$只由模型当前的隐藏状态$h^{(t)}$决定。<br>    4）$L^{(t)}$代表在序列索引号$t$时模型的损失函数。<br>    5）$y^{(t)}$代表在序列索引号$t$时训练样本序列的真实输出。<br>    6）$U,W,V$这三个矩阵是我们的模型的线性关系参数，它在整个RNN网络中是共享的，这点和DNN很不相同。<br>    也正因为是共享了，它体现了RNN的模型的“循环反馈”的思想。　</p>
<h1 id="3-RNN前向传播算法"><a href="#3-RNN前向传播算法" class="headerlink" title="3.RNN前向传播算法"></a>3.RNN前向传播算法</h1><h1 id="4-RNN反向传播算法推导"><a href="#4-RNN反向传播算法推导" class="headerlink" title="4.RNN反向传播算法推导"></a>4.RNN反向传播算法推导</h1><h1 id="5-RNN小结"><a href="#5-RNN小结" class="headerlink" title="5.RNN小结"></a>5.RNN小结</h1><pre><code>上面调整参数的细节还没有了解之外，其他任务也算是了解得七七八八了吧，至于上面欠缺的图2，图3，我想等我真的学到的时候再进行添加吧。
RNN虽然理论上可以很好德解决序列数据的训练，但是它也像DNN一样有梯度小时时的问题，当序列很长的时候问题尤其严重。因此，上面的RNN模型一般不能直接用于应用领域。手写识别以及机器翻译等NLP领域实际应用比较广泛的事基于RNN模型的一个特例LSTM，接下来，将做LSTM的记录。由于目前LSTM的tensorflow版已经记录完成了，接下来的工作就是记录好LSTM的技术理论和流程。
</code></pre><p>ps：内容基本上都是抄的网络上和书上的，自己总结的东西还是少，只能说任重而道远了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/14/rnn/" data-id="cjjcsffgk0008ccnw1lywy5l8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-perceptron" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/07/perceptron/" class="article-date">
  <time datetime="2018-06-07T03:00:57.000Z" itemprop="datePublished">2018-06-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/07/perceptron/">perceptron 感知机</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<h1 id="题记"><a href="#题记" class="headerlink" title="题记"></a>题记</h1><p>  最近终于能有一些时间空下来来整理一下这学期学到的东西了， 以后每天都会坚持写一写，就算是以后工作了，也会坚持写（我是这么立下flag的）。<br>  目前最需要整理的就是深度学习的东西，有一些深入的内容后面再学的时候会慢慢更新博客（好吧，由于也没人看，我自己跟自己说话，说得这么起劲）。<br>  关于深度学习的内容，基本上没一个内容是写得好的，然而既然我已经厚颜无耻地要写博客了，那我就尽力写一写，假装自己能写得不错，然后再洋洋得意一番。<br>  由于我是用tensorflow实践的深度学习，图也基本上是照着网上扒拉下来的，所以如果有侵权警告的话，请github给我留言。<br>  废话不说了，开始今天的内容吧。</p>
<h1 id="什么是感知机？"><a href="#什么是感知机？" class="headerlink" title="什么是感知机？"></a>什么是感知机？</h1><pre><code>这个题目怕是已经被写烂了，毕竟是整个体系里面相对来说比较简单的，同时也是入门模型。
</code></pre><p>  在介绍感知机之前，首先介绍一下二分类。<br><img src="https://iajqs.github.io/img/LSTM3-chain.png" alt=""><br></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/07/perceptron/" data-id="cjjcsfffk0001ccnw54eu4q9s" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-以太坊Dapp开发环境配置与实践" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/06/以太坊Dapp开发环境配置与实践/" class="article-date">
  <time datetime="2018-05-06T03:34:55.000Z" itemprop="datePublished">2018-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/06/以太坊Dapp开发环境配置与实践/">以太坊Dapp开发环境配置与实践</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、工具"><a href="#一、工具" class="headerlink" title="一、工具"></a>一、工具</h1><h2 id="需要安装的工具列表"><a href="#需要安装的工具列表" class="headerlink" title="需要安装的工具列表"></a>需要安装的工具列表</h2><p>如果你不喜欢把时间浪费在搭建开发环境上，可以使用汇智网的在线练习环境及教程：以太坊 DApp 开发实战入门–<a href="http://xc.hubwiz.com/course/5a952991adb3847553d205d1?affid=v2ex7878" target="_blank" rel="noopener">http://xc.hubwiz.com/course/5a952991adb3847553d205d1?affid=v2ex7878</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cnpm</span><br><span class="line">Node.js</span><br><span class="line">Geth</span><br><span class="line">solc</span><br><span class="line">web3</span><br><span class="line">truffle</span><br><span class="line">webpack</span><br></pre></td></tr></table></figure></p>
<h2 id="安装cnpm"><a href="#安装cnpm" class="headerlink" title="安装cnpm"></a>安装cnpm</h2><p>使用淘宝镜像安装， 速度会比较快<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure></p>
<p>安装完成后创建一个软链接<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; sudo ln -s /home/ubuntu/node-v4.5.0-linux-x86/bin/cnpm /usr/<span class="built_in">local</span>/bin/</span><br></pre></td></tr></table></figure></p>
<p>这样就可以在全局使用cnpm了</p>
<p>查看<code>cnpm</code>版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cnpm -v</span><br></pre></td></tr></table></figure></p>
<h2 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h2><p>直接登录官网下载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">官网地址：http://nodejs.cn/</span><br></pre></td></tr></table></figure></p>
<p>下载后直接安装即可。安装完毕，打开控制台窗口，查看nodejs版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; node -v</span><br></pre></td></tr></table></figure></p>
<h2 id="安装Geth"><a href="#安装Geth" class="headerlink" title="安装Geth"></a>安装Geth</h2><p>登录官网下载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下载地址：https://geth.ethereum.org/downloads/</span><br></pre></td></tr></table></figure></p>
<p>下载安装后，查看geth版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; geth version</span><br></pre></td></tr></table></figure></p>
<h2 id="安装solidity编译器"><a href="#安装solidity编译器" class="headerlink" title="安装solidity编译器"></a>安装solidity编译器</h2><p>使用cnpm安装，npm是真的强大，可以直接安装好多东西<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; npm install -g solc</span><br></pre></td></tr></table></figure></p>
<p>安装完毕后， 执行命令验证安装成功<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; solcjs -version</span><br></pre></td></tr></table></figure></p>
<p>执行出错，没有解决。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; Must provide a file</span><br></pre></td></tr></table></figure></p>
<h2 id="安装web3"><a href="#安装web3" class="headerlink" title="安装web3"></a>安装web3</h2><p>由于我使用的操作系统是git，需要先安装windows版的git命令行。<br>在<code>git bash</code>控制台输入以下命令(在这里卡了好久，一直以为是再cmd里输入一下命令)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cnpm install -g web3@0.20.2</span><br></pre></td></tr></table></figure></p>
<p>如果直接使用npm进行安装，会报错，可能是需要翻墙的关系。</p>
<h2 id="安装truffle"><a href="#安装truffle" class="headerlink" title="安装truffle"></a>安装truffle</h2><p>使用cnpm安装truffle开发框架<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cnpm install -g truffle</span><br></pre></td></tr></table></figure></p>
<p>查看truffle版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; truffle.cmd version</span><br></pre></td></tr></table></figure></p>
<p>windows系统下， 使用truffle要加上.cmd后缀，即使用windows的内核脚本文件，具体原因有待探究</p>
<h2 id="安装webpack"><a href="#安装webpack" class="headerlink" title="安装webpack"></a>安装webpack</h2><p>使用cnpm安装webpack<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cnpm install -g webpack@3.11.0</span><br></pre></td></tr></table></figure></p>
<p>查看webpack版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; webpack -v</span><br></pre></td></tr></table></figure></p>
<h1 id="二、运行私链节点"><a href="#二、运行私链节点" class="headerlink" title="二、运行私链节点"></a>二、运行私链节点</h1><h2 id="2-1-创世块配置"><a href="#2-1-创世块配置" class="headerlink" title="2.1 创世块配置"></a>2.1 创世块配置</h2><p>创建一个文件夹，在其中创建私链的创世块配置文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; mkdir node1</span><br><span class="line">&gt; <span class="built_in">cd</span> node1</span><br><span class="line">\node1&gt; notepad private.json</span><br></pre></td></tr></table></figure></p>
<p>然后编辑内容如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"config"</span>:&#123;</span><br><span class="line">        <span class="attr">"chainId"</span>:<span class="number">7878</span>,</span><br><span class="line">        <span class="attr">"homesteadBlock"</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="attr">"eip155Block"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"eip158Block"</span>:<span class="number">0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    "difficulty":"200",     //挖矿难度，值越小难度越小</span><br><span class="line">    "gasLimit":"0xffffffff", //油费上限值，由于我们是私链，所以上限值设置为最大</span><br><span class="line">    "alloc":&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>config.chainId</code>：用来声明以太坊网络编号，选择一个大于10的数字即可<br><code>difficulty</code>：用来声明挖矿难度，越小的值难度越低，也就能更快速地出块<br><code>gasLimit</code>：油费上限值，在区块链上进行交易，需要消耗gas，如果需要消耗的gas超过了gasLimit，区块链将退回交易请求，不允执行</p>
<h2 id="初始化私链节点"><a href="#初始化私链节点" class="headerlink" title="初始化私链节点"></a>初始化私链节点</h2><p>执行geth的init命令初始化私链节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\node1&gt; geth --datadir .\data init private.json</span><br></pre></td></tr></table></figure></p>
<p>这会在node1目录下创建data目录，用来保存区块数据及账户信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\node1&gt; dir</span><br><span class="line">data private.json</span><br></pre></td></tr></table></figure></p>
<p>将上述命令写入一个脚本init.cmd里，便于下次初始化私链节点<br>内容如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">geth --data .\data init private.json</span><br></pre></td></tr></table></figure></p>
<h2 id="2-3-启动私链节点"><a href="#2-3-启动私链节点" class="headerlink" title="2.3 启动私链节点"></a>2.3 启动私链节点</h2><p>从指定的私链数据目录启动并设定一个不同的网络编号来启动节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\node1&gt; geth --rpc --rpcaddr 0.0.0.0 --rpccorsdomain <span class="string">"*"</span> --datadir .\data --networdid 7878 console</span><br></pre></td></tr></table></figure></p>
<p>这个也同样可以使用一个脚本console.cmd来保存以上的命令，便于下次启动私链节点</p>
<p>以后启动节点，只要直接执行这个脚本即可<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\node1&gt; console.cmd</span><br></pre></td></tr></table></figure></p>
<h2 id="2-4-账户管理"><a href="#2-4-账户管理" class="headerlink" title="2.4 账户管理"></a>2.4 账户管理</h2><h4 id="2-4-1查看账户列表"><a href="#2-4-1查看账户列表" class="headerlink" title="2.4.1查看账户列表"></a>2.4.1查看账户列表</h4><p>首先， 执行<code>console.cmd</code>进入geth控制台，使用eth对象的accounts属性查看目前的账户列表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; eth.accounts</span><br><span class="line">[]</span><br></pre></td></tr></table></figure></p>
<p>这个时候列表是空的， 我们需要创建一个用户</p>
<h4 id="2-4-2创建新账户"><a href="#2-4-2创建新账户" class="headerlink" title="2.4.2创建新账户"></a>2.4.2创建新账户</h4><p>在geth控制台， 使用personal对象的newAccount()方法创建一个新账户，参数为密码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; personal.newAccount(<span class="string">'123456'</span>)</span><br><span class="line"><span class="string">"0x79a1c7391c620882cebaffaf7b86e0c8236679a5"</span></span><br></pre></td></tr></table></figure></p>
<p>输出就是新创建的账户地址（公钥），你的输出不会和上面的示例相同。geth会保存到数据目录下的keystore文件中。</p>
<h4 id="2-4-3查询账户余额"><a href="#2-4-3查询账户余额" class="headerlink" title="2.4.3查询账户余额"></a>2.4.3查询账户余额</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; eth.getBalance(eth.accounts[0])</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<p>新创建的账户， 余额为0</p>
<h4 id="2-4-4挖矿"><a href="#2-4-4挖矿" class="headerlink" title="2.4.4挖矿"></a>2.4.4挖矿</h4><p>在geth控制台， 执行miner对象的start()方法来启动挖矿<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; miner.start(1)</span><br></pre></td></tr></table></figure></p>
<p>其中<code>start(1)</code>中的<code>1</code>表示开一个线程进行挖矿，如果不配置，就会让CPU全速运行，影响计算机的使用。</p>
<ul>
<li>运行一会后，主账号就会获取很多以太币，这个时候屏幕会快速刷屏，不用管，输入命令miner.stop()停止挖矿。</li>
<li>这里有个需要注意的地方，如果是第一次挖矿，需要等待加载到100才会开始挖矿，请耐心等待<br>看到挖出一些块之后，检查账户余额：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; eth.getBalance(eth.accounts[0])</span><br><span class="line">225000000000000000000</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>停止挖矿<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; miner.stop()</span><br></pre></td></tr></table></figure></p>
<h4 id="2-4-5解锁账户"><a href="#2-4-5解锁账户" class="headerlink" title="2.4.5解锁账户"></a>2.4.5解锁账户</h4><p>在部署合约是需要一个解锁的账户。<br>在geth控制台使用personal对象的unlockAccount()方法来解锁指定的账户，参数为账户地址和账户密码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; eth.unlockAccount(eth.accounts[0],<span class="string">"123456"</span>)</span><br></pre></td></tr></table></figure></p>
<p>三、构建示例项目</p>
<h4 id="3-1新建DApp项目"><a href="#3-1新建DApp项目" class="headerlink" title="3.1新建DApp项目"></a>3.1新建DApp项目</h4><p>执行一下命令创建项目目录并进入该目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; mkdir demo</span><br><span class="line">&gt; <span class="built_in">cd</span> demo</span><br></pre></td></tr></table></figure></p>
<p>接着使用truffle初始化项目框架结构<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; truffle.cmd unbox webpack</span><br></pre></td></tr></table></figure></p>
<p>很多教程是用<code>truffle init</code>,但是生成的项目会有一些问题，所以建议使用以上命令</p>
<h4 id="3-2安装项目依赖的NPM包"><a href="#3-2安装项目依赖的NPM包" class="headerlink" title="3.2安装项目依赖的NPM包"></a>3.2安装项目依赖的NPM包</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; npm install</span><br></pre></td></tr></table></figure>
<h4 id="3-3修改truffle配置"><a href="#3-3修改truffle配置" class="headerlink" title="3.3修改truffle配置"></a>3.3修改truffle配置</h4><p>打开truffle.js配置文件，内容如下：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Allows us to use ES6 in our migrations and tests.</span></span><br><span class="line"><span class="built_in">require</span>(<span class="string">'babel-register'</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  networks: &#123;</span><br><span class="line">    development: &#123;</span><br><span class="line">      host: <span class="string">'127.0.0.1'</span>,</span><br><span class="line">      port: <span class="number">8545</span>,</span><br><span class="line">      network_id: <span class="string">'*'</span> <span class="comment">// Match any network id</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>其中<code>host:127.0.0.1</code>等同于<code>host:localhost</code></li>
<li>端口号<code>port</code>需要根据私链所在的端口号来设置， 如果没有特别制定，geth默认端口号为8545</li>
</ul>
<h4 id="3-4启动节点"><a href="#3-4启动节点" class="headerlink" title="3.4启动节点"></a>3.4启动节点</h4><p>部署合约之前，我们先启动节点，直接执行我们上面保存的<code>console.cmd</code></p>
<h4 id="3-5编译合约"><a href="#3-5编译合约" class="headerlink" title="3.5编译合约"></a>3.5编译合约</h4><p>执行一下命令编译项目合约<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; truffle.cmd compile</span><br></pre></td></tr></table></figure></p>
<h4 id="3-6部署合约"><a href="#3-6部署合约" class="headerlink" title="3.6部署合约"></a>3.6部署合约</h4><p>执行一下命令来部署合约：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; truffle.cmd migrate</span><br></pre></td></tr></table></figure></p>
<p>执行部署合约之前要记得解锁账户，不然会报错。错误信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: authentication needed: password or unlock</span><br></pre></td></tr></table></figure></p>
<p>如果已经正确地解锁了账户，你会看到部署过程停止在如下状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Replacing Migrations...</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>这是因为部署合约其实就是一个交易，只要是交易就需要记录到区块链中，这个时候就需要挖矿来记录交易信息。</p>
<p>返回geth终端窗口，查看交易池的状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; txpool.status</span><br><span class="line">&#123;</span><br><span class="line">    pending:1,</span><br><span class="line">    queued:0 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>显示有一个挂起的交易，开始挖矿：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; miner.start(1)</span><br></pre></td></tr></table></figure></p>
<p>挖出一定区块后，再查看交易池的状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; txpool.status</span><br><span class="line">&#123;</span><br><span class="line">    pending:0,</span><br><span class="line">    queued:0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>交易提交完成，停止挖矿<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; miner.stop()</span><br></pre></td></tr></table></figure></p>
<h4 id="3-7启动DApp"><a href="#3-7启动DApp" class="headerlink" title="3.7启动DApp"></a>3.7启动DApp</h4><p>一开始，我看的教程是使用一下命令来启动<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; truffle deploy</span><br><span class="line">\demo&gt; truffle serve</span><br></pre></td></tr></table></figure></p>
<p>但是，这样会报这样的错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Serving static assets <span class="keyword">in</span> .\build on port 8080...</span><br><span class="line">TypeError: fsevents is not a constructor</span><br></pre></td></tr></table></figure></p>
<p>一些文章的解释说，是因为<code>truffle</code>的版本太高，建议删除并安装<a href="mailto:`truffle@3.2.1" target="_blank" rel="noopener">`truffle@3.2.1</a><code>，执行制定的操作后，发现安装不上</code><a href="mailto:truffle@3.2.1" target="_blank" rel="noopener">truffle@3.2.1</a>`，宣告失败，最后找到一个别的方法启动，代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; npm run dev</span><br></pre></td></tr></table></figure></p>
<p>在浏览器里访问<a href="http://localhost:8081" target="_blank" rel="noopener">http://localhost:8081</a> 即可，我的端口号是8081，汇智网教程的是8080，以你输入<code>npm run dev</code>之后控制台打印的网址为准。<br><img src="https://iajqs.github.io/img/MetaCoin界面.jpg" alt=""><br>在最后的页面显示中，还是会报错，错误信息如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;There was an error fetching your accounts.&quot;</span><br></pre></td></tr></table></figure></p>
<p>打开控制台，发现显示无法链接<code>127.0.0.1:9545</code>，打开项目文件夹下的app\javascripts，打开app.js，找到以下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.web3 = <span class="keyword">new</span> Web3(<span class="keyword">new</span> Web3.providers.HttpProvider(<span class="string">"http://127.0.0.1:9545"</span>));</span><br></pre></td></tr></table></figure></p>
<p>改为<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.web3 = <span class="keyword">new</span> Web3(<span class="keyword">new</span> Web3.providers.HttpProvider(<span class="string">"http://localhost:8545"</span>));</span><br></pre></td></tr></table></figure></p>
<p>刷新网页，发现还有错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: The method net_version does not exist/is not available</span><br></pre></td></tr></table></figure></p>
<p>暂时没有解决，学业不精，内牛满面。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/05/06/以太坊Dapp开发环境配置与实践/" data-id="cjjcsffgn0009ccnwpupfrp4y" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/区块链/">区块链</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-plan" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/03/plan/" class="article-date">
  <time datetime="2018-05-03T01:19:28.000Z" itemprop="datePublished">2018-05-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/03/plan/">记录</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="需完善博客"><a href="#需完善博客" class="headerlink" title="需完善博客"></a>需完善博客</h1><ol>
<li><a href="../../02/LSTM/">LSTM</a></li>
</ol>
<h1 id="blockChain"><a href="#blockChain" class="headerlink" title="blockChain"></a>blockChain</h1><ul>
<li>打印以太零白皮书</li>
<li>完成以太零部署智能合约（发布token）</li>
<li>完成token发布后， 撰写整个发布流程</li>
</ul>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><ol>
<li>回实验室记录idea</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/05/03/plan/" data-id="cjjcsffgi0007ccnwtxfstg70" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-LSTM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/02/LSTM/" class="article-date">
  <time datetime="2018-05-02T11:27:22.000Z" itemprop="datePublished">2018-05-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/02/LSTM/">LSTM-tensorflow</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="LSTM简介（抄自《tensorflow实战》）"><a href="#LSTM简介（抄自《tensorflow实战》）" class="headerlink" title="LSTM简介（抄自《tensorflow实战》）"></a>LSTM简介（抄自《tensorflow实战》）</h1><p>LSTM是Schmidhuber教授于1997年提出，它天生就是为了解决长程依赖而设计的，不需要特别复杂地调试超参数，默认就可以记住长期的信息。LSTM的内部结构相比RNN更复杂，如图1-1所示，其中包含了4层神经网络，其中小圆圈是point-wise的操作，比如向量加法、点乘等，而小矩形则代表一层可以学习参数的神经网络。LSTM单元上面的那条直线代表了LSTM的状态state，它会贯穿所有串联在一起的LSTM单元，从第一个LSTM单元一直流向最后一个LSTM单元，其中只有少量的线性干预和改变。状态state在这条隧道中传递时，LSTM单元可以对其添加或删减信息，这些对信息流的修改操作由LSTM中的Gates控制。这些Gates中包含了一个Sigmoid层和一个向量点乘的操作，这个Sigmoid层的输出是0到1之间的值（这个sigmoid的长相还挺奇怪呀），它直接控制了信息传递的比例。如果为0代表不允许信息传递，为1则代表让信息全部通过。每个LSTM单元中包含了3个这样的Gates，用来维护和控制单元的状态信息。凭借对状态信息的储存和修改，LSTM单元就可以实现长程记忆<br><img src="https://iajqs.github.io/img/LSTM3-chain.png" alt=""><br></p>
<p><center>图1 LSTM结构示意图</center><br><img src="https://iajqs.github.io/img/LSTM2-notation.png" alt=""><br></p>
<h1 id="LSTM的tensorflow实现和注解"><a href="#LSTM的tensorflow实现和注解" class="headerlink" title="LSTM的tensorflow实现和注解"></a>LSTM的tensorflow实现和注解</h1><p>下面只讲解主要的部分，其他内容请查看《tensorflow实战》</p>
<h4 id="1-导入数据："><a href="#1-导入数据：" class="headerlink" title="1.导入数据："></a>1.导入数据：</h4><ul>
<li>首先下载<code>PTB</code>数据并解压<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz</span><br><span class="line">tar xvf simple-examples.tgz</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>确保解压后的文件路径和接下来Python的执行路径一致。</p>
<ul>
<li><p>下载<code>TensorFlow Models</code>库，并进入目录<code>models/tutorials/rnn/ptb</code>，将其中的<code>reader.py</code>复制到库中<code>(site-packages)</code>，借助它读取数据内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</span><br><span class="line"><span class="built_in">cd</span> models/tutorials/rnn/ptb</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后导入<code>reader</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> reader</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-创建模型"><a href="#2-创建模型" class="headerlink" title="2.创建模型"></a>2.创建模型</h4><h4 id="构建LSTM单元"><a href="#构建LSTM单元" class="headerlink" title="构建LSTM单元"></a>构建LSTM单元</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">### size = hidden_size</span></span><br><span class="line">    <span class="comment">### state_is_tuple mean 2-tuple 也就是二元</span></span><br><span class="line">    <span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</span><br><span class="line">        size, forget_bias = <span class="number">0.0</span>, state_is_tuple = <span class="keyword">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment">## set the lstm_cell function to attn_cell</span></span><br><span class="line">attn_cell = lstm_cell</span><br><span class="line"><span class="comment">## if training and the config.keep_prob &lt; 1</span></span><br><span class="line"><span class="comment">### then add the Dropout layer to the lstm_cell</span></span><br><span class="line"><span class="keyword">if</span> is_training <span class="keyword">and</span> config.keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">attn_cell</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="comment">#### let the lstm_cell()'s output as the input for DropoutWrapper</span></span><br><span class="line">        <span class="keyword">return</span> tf.contrib.rnn.DropoutWrapper(</span><br><span class="line">            lstm_cell(), output_keep_prob = config.keep_prob</span><br><span class="line">        )</span><br><span class="line"><span class="comment">## pile the lstm_cell (i think here should be attn_cell) num_layers times</span></span><br><span class="line">cell = tf.contrib.rnn.MultiRNNCell(</span><br><span class="line">    [attn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(config.num_layers)],</span><br><span class="line">    state_is_tuple = <span class="keyword">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self._initial_state = cell.zero_state(batch_size, tf.float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>这里需要注意，<code>LSTM单元</code>可以读入一个单词并结合之前储存的状态<code>state</code>计算下一个单词出现的概率分布，并且每次读取一个单词后它的状态<code>state</code>会被更新。</li>
<li><font color="#FF0000">这里我并不是很理解使用<code>rnn.MultiRNNCell</code>将前面的<code>lstm_cell</code>多层堆叠到<code>cell</code>具体是什么意思，难道是说两个<code>LSTM单元</code>连接在一起（参考图1-1）吗？暂时只能这么理解了。</font></li>
<li><p><font color="#FF0000">并不理解<code>tf.contrib.rnn.DropoutWrapper</code>的具体作用,难道是所谓的：</font></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">状态state在这条隧道中传递时，LSTM单元可以对其添加或删减信息，这些对信息流的修改操作由LSTM中的Gates控制。</span><br></pre></td></tr></table></figure>
<ul>
<li>然而我依旧不能下定论</li>
</ul>
</li>
</ul>
<h4 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h4><font color="#FF0000">测试</font><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## limit the caculate operation in cpu</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/cpu:0"</span>):</span><br><span class="line">    <span class="comment">### embedding the data</span></span><br><span class="line">    embedding = tf.get_variable(</span><br><span class="line">        <span class="string">"embedding"</span>, [vocab_size, size], dtype=tf.float32</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">### get the embed of input_.input_data from embedding</span></span><br><span class="line">    <span class="comment">#### inputs: inputs[batch_size, word_size, vector_size]</span></span><br><span class="line">    &lt;!-- 晚上回去测试一下 --&gt;</span><br><span class="line">    <span class="comment">#### batch_size = config.batch_size</span></span><br><span class="line">    <span class="comment">#### word_size = config.num_steps</span></span><br><span class="line">    <span class="comment">#### vector_size = config.vector_size</span></span><br><span class="line">    inputs = tf.nn.embedding_lookup(embedding, input_.input_data)</span><br></pre></td></tr></table></figure><br><br><em> 注这里果然还是需要测试一下<code>input_.input_data</code>的数据是什么样的，我怀疑<code>PTBinput</code>返回的是想<code>Word2Vec</code>那样的<code>top</code>编号<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## if is_training and keep_prob &lt; 1, add the dropout</span></span><br><span class="line"><span class="comment">### the dropout(inputs, keep_prob) is mean to get (keep_prob / 1) x 100% data from input</span></span><br><span class="line"><span class="keyword">if</span> is_training <span class="keyword">and</span> config.keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">    inputs = tf.nn.dropout(inputs, config.keep_prob)</span><br></pre></td></tr></table></figure>

</em> 这里发现， 对输入数据也进行了<code>dropout</code>操作，得到<code>新的inputs</code>，后面的操作中， 将会把这个<code>新的inputs</code>输入给<code>cell</code>,那岂不是进行了两次<code>dropout</code><br><br>#### 使用模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">outputs = []</span><br><span class="line">state = self._initial_state</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"RNN"</span>):</span><br><span class="line">    <span class="comment">### set the loop size is num_steps</span></span><br><span class="line">    <span class="keyword">for</span> time_step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        <span class="comment">### at the sencond loop , use the tf.get_variable_scope().reuse_variables() set the reuse variables</span></span><br><span class="line">        <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">            tf.get_variable_scope().reuse_variables()</span><br><span class="line">        <span class="comment">### input the (inputs, state) to the LSTM cell</span></span><br><span class="line">        <span class="comment">### get the result(cell_output) and the updated state</span></span><br><span class="line">        <span class="comment">#### input[a, b, c]</span></span><br><span class="line">            <span class="comment">##### a: the index of sample in one batch</span></span><br><span class="line">            <span class="comment">##### b: the index of word in one sample</span></span><br><span class="line">            <span class="comment">##### c: the dimension number of the word's vector</span></span><br><span class="line">        (cell_output, state) = cell(inputs[:, time_step, :], state)</span><br><span class="line">        <span class="comment">### add the result to the output list</span></span><br><span class="line">        outputs.append(cell_output)</span><br></pre></td></tr></table></figure><br><br><em> <code>num_step</code> 反向传播时可以展开的步数
</em> <code>(cell_output, state) = cell(inputs[:, time_step, :], state)</code> 其中 <code>input[a, b, c]</code> 的a表示batch中的第几个样本，b表示样本中的第几个单词，c表示单词的向量表达式的维度，<code>inputs[:, time_step, :]</code>而这里表示的是所有样本的第time_step个单词。<font color="#FF0000">（这个输入好特别， 又是一个需要研究一下的点）</font>

<h4 id="设置模型配置信息"><a href="#设置模型配置信息" class="headerlink" title="设置模型配置信息"></a>设置模型配置信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># init the SamllConfig, ok, this is according with the modelize programing</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SmallConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">## init the weigth of the net</span></span><br><span class="line">    init_scale = <span class="number">0.1</span></span><br><span class="line">    <span class="comment">## set the learning rate</span></span><br><span class="line">    learning_rate = <span class="number">1.0</span></span><br><span class="line">    <span class="comment">## set the gradient's max norm</span></span><br><span class="line">    max_grad_norm = <span class="number">5</span></span><br><span class="line">    <span class="comment">## set the Stackingable layers</span></span><br><span class="line">    num_layers = <span class="number">2</span></span><br><span class="line">    <span class="comment">## LSTM gradient backpropagation's step</span></span><br><span class="line">    num_steps = <span class="number">20</span></span><br><span class="line">    <span class="comment">## the number of hidden node</span></span><br><span class="line">    hidden_size = <span class="number">200</span></span><br><span class="line">    <span class="comment">## initial learning rate could train times</span></span><br><span class="line">    max_epoch = <span class="number">4</span></span><br><span class="line">    <span class="comment">## altogether cound train times</span></span><br><span class="line">    max_max_epoch = <span class="number">13</span></span><br><span class="line">    <span class="comment">## dropout layer retain proportion</span></span><br><span class="line">    keep_prob = <span class="number">1.0</span></span><br><span class="line">    <span class="comment">## this is the learning rate's decay speed</span></span><br><span class="line">    lr_decay = <span class="number">0.5</span></span><br><span class="line">    <span class="comment">## batch_size is batch's size</span></span><br><span class="line">    batch_size = <span class="number">20</span></span><br><span class="line">    <span class="comment">## vocabulary's size</span></span><br><span class="line">    vocab_size = <span class="number">10000</span></span><br></pre></td></tr></table></figure>
<h4 id="3-训练"><a href="#3-训练" class="headerlink" title="3.训练"></a>3.训练</h4><ul>
<li>其中输入为session会话， </li>
<li>model模型</li>
<li>eval_op是否有测评操作<font color="#FF0000">（暂时不知道是为什么)</font></li>
<li><p>verbose是一个打印标记</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(session, model, eval_op = None, verbose = False)</span>:</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    costs = <span class="number">0.0</span></span><br><span class="line">    iters = <span class="number">0</span></span><br><span class="line">    state = session.run(model.initial_state)</span><br><span class="line">    <span class="comment">## create the fetches to get the run results</span></span><br><span class="line">    fetches = &#123;</span><br><span class="line">        <span class="string">"cost"</span>: model.cost,</span><br><span class="line">        <span class="string">"final_state"</span>: model.final_state</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> eval_op <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        fetches[<span class="string">"eval_op"</span>] = eval_op</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(model.input.epoch_size):</span><br><span class="line">        feed_dict = &#123;&#125;</span><br><span class="line">        <span class="comment">### get the LSTM's all state to feed_dict</span></span><br><span class="line">        <span class="comment">#### enumerate(["a", "b", "c"]) ==&gt; [(0, "a"), (1, "b"), (2, "c")]</span></span><br><span class="line">        <span class="comment">#### ok, now, I don's konw what is the c or h</span></span><br><span class="line">        <span class="keyword">for</span> i, (c, h) <span class="keyword">in</span> enumerate(model.initial_state):</span><br><span class="line">            feed_dict[c] = state[i].c</span><br><span class="line">            feed_dict[h] = state[i].h</span><br><span class="line"></span><br><span class="line">        <span class="comment">### run and get the result</span></span><br><span class="line">        vals = session.run(fetches, feed_dict)</span><br><span class="line">        cost = vals[<span class="string">"cost"</span>]</span><br><span class="line">        state = vals[<span class="string">"final_state"</span>]</span><br><span class="line"></span><br><span class="line">        costs += cost</span><br><span class="line">        iters += model.input.num_steps</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> verbose <span class="keyword">and</span> step % (model.input.epoch_size // <span class="number">10</span>) == <span class="number">10</span>:</span><br><span class="line">            print(<span class="string">"%.3f perplexity: %.3f speed: %.0f wps"</span> %</span><br><span class="line">                  (step * <span class="number">1.0</span>/model.input.epoch_size, np.exp(costs/iters),</span><br><span class="line">                   iters * model.input.batch_size / (time.time() - start_time)))</span><br><span class="line">    <span class="keyword">return</span> np.exp(costs / iters)</span><br></pre></td></tr></table></figure>
</li>
<li><font color="#FF0000"><code>feed_dict[c] = state[i].c</code> 和 <code>state[i].h</code>都不知道具体指的是什么</font></li>
<li>其他的还ok</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/05/02/LSTM/" data-id="cjjcsfffn0002ccnwmc465vfc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Skip-Gram" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/30/Skip-Gram/" class="article-date">
  <time datetime="2018-04-30T07:24:22.887Z" itemprop="datePublished">2018-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>title: Skip-Gram-tensorflow<br>date: 2018-04-30 15:24:22</p>
<h2 id="tags-深度学习"><a href="#tags-深度学习" class="headerlink" title="tags: 深度学习"></a>tags: 深度学习</h2><h1 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h1><p>tensorflow<br>pycharm</p>
<h1 id="什么是Word2Vec和Embeddings？"><a href="#什么是Word2Vec和Embeddings？" class="headerlink" title="什么是Word2Vec和Embeddings？"></a>什么是Word2Vec和Embeddings？</h1><p>Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。</p>
<p>我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy这个词向量十分相近。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="../../../../img/Word2Vec.jpg" alt=""><br><br>Skip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。</p>
<p>Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。</p>
<h1 id="从tensorflow上得到需要注意的细节"><a href="#从tensorflow上得到需要注意的细节" class="headerlink" title="从tensorflow上得到需要注意的细节"></a>从tensorflow上得到需要注意的细节</h1><p>《tensorflow实战》的例程：<br>1.先统计训练文本中的所有单词的tf值<br>2.按照tf值进行排序，并用每个词在排序表中下标来表示这个词<br>3.使用embeding的方式将下标转换成一个128为的随机矩阵， 然后后面会通过计算损失函数自动更新这个矩阵， 这个embeding矩阵就代表着这个词<br>4.最后会得到一个训练好的embeding表<br>5.可以通过计算某一个词的下标在embeding表中找到与之相似度较高的词下标</p>
<hr>
<h1 id="根据和学长们的讨论，这里很有必要追加一些内容"><a href="#根据和学长们的讨论，这里很有必要追加一些内容" class="headerlink" title="根据和学长们的讨论，这里很有必要追加一些内容"></a>根据和学长们的讨论，这里很有必要追加一些内容</h1><h4 id="这个Fake-Task是训练什么？计算的损失是谁的损失？"><a href="#这个Fake-Task是训练什么？计算的损失是谁的损失？" class="headerlink" title="这个Fake Task是训练什么？计算的损失是谁的损失？"></a>这个Fake Task是训练什么？计算的损失是谁的损失？</h4><p>  前面我们已经知道，我们会将词转换成embeding的形式，然后交给模型去训练，最后我们会得到一个训练好的embeding表。<br>  从整体上来看，这个embeding是一个权重矩阵，它是某个词的特征码（这么说的原因是为了体现特征码接近的词，特征也比较接近）。<br>  而训练过程中调整的模型（skip-gram）的作用是，输入一个词向量，会输出一个与词向量同等规模的向量。<br>  这个向量不一定代表某个词，但可以与正确的输出结果计算损失，然后通过这个损失调整词向量和模型里的参数。emmm，从整体上来看，这个词向量其实也是参数之一。<br>  那么，说到这里，总结一下，Fake Task的真正意图是调整词向量，也就是所谓的权重矩阵，而这个模型本身具有的测试功能除了调整权重矩阵之外，没有别的用途（只就这里来讲）。计算的损失是skip-gram模型的损失，但这个模型后面不会再使用。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/30/Skip-Gram/" data-id="cjjcsfffd0000ccnwb3m57dnw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/区块链/">区块链</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/区块链/" style="font-size: 10px;">区块链</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/深度学习/" style="font-size: 20px;">深度学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/07/08/机器学习第四章/">机器学习第四章</a>
          </li>
        
          <li>
            <a href="/2018/07/06/机器学习第三章/">机器学习第三章</a>
          </li>
        
          <li>
            <a href="/2018/07/06/机器学习第二章/">机器学习第二章</a>
          </li>
        
          <li>
            <a href="/2018/07/05/机器学习第一章/">机器学习第一章</a>
          </li>
        
          <li>
            <a href="/2018/06/14/rnn/">rnn</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>