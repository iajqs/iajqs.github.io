<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-RNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/14/RNN/" class="article-date">
  <time datetime="2018-06-14T01:26:04.000Z" itemprop="datePublished">2018-06-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/14/RNN/">rnn</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <pre><code>Word2vec理解得比较清晰了，虽然关于CBOW中使用的哈夫曼树的实际作用和操作并没有完全理解清楚，但我感觉那不是我现在应该纠结的东西，我现在的目标是先把已经学了的东西掌握起来，至于这些更加深层的模型优化算法，以后再说了，所以我现在之类存个记录，为word2vec加入一个待完善的标记。
这一篇将讨论RNN，关于RNN的模型和一些应用。
</code></pre><h1 id="1-RNN概述"><a href="#1-RNN概述" class="headerlink" title="1.RNN概述"></a>1.RNN概述</h1><pre><code>像CNN，训练样本的输入和输出都是比较确定的。CNN能自行提取特征，性能非常优秀，但是如果训练样本是连续的序列，且序列的长短不一，比如基于时间的序列，一段文字等等，我们将主要讨论文字层面的内容。
下面我们来看RNN的模型。
</code></pre><h1 id="2-RNN模型"><a href="#2-RNN模型" class="headerlink" title="2.RNN模型"></a>2.RNN模型</h1><pre><code>这里我直接照搬深度学习（圣经）里的内容。
循环神经网络中一些重要的设计模式包括一下几种：
（1）每个时间步都有输出，并且隐藏单元之间有循环链接的循环网络，如图1所示。
（2）每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环链接的循环网络，如图2所示。（不懂）
（3）隐藏单元之间存在会循环链接，但读取整个序列后产生单个输出的循环网络，如图3所示。（不懂）
</code></pre><p>ps：我的天，除了第一种，其他的都不知道怎么回事。<br><img src="https://iajqs.github.io/img/RNN1.png" alt=""><br><br>    上图中左边是RNN模型没有按时间展开的图，如果按时间序列展开，则是上图中的右边部分。我们重点观察右边部分的图。<br>    这幅图描述了在序列索引号$t$附近RNN的模型。其中：<br>    1）$x^{(t)}$代表在序列索引号$t$时训练样本的输入。同样的，$x^{(t-1)}$和$x^{(t+1)}$代表在序列索引号$t-1$和$t+1$时训练样本的输入。<br>    2）$h^{(t)}$代表在序列索引号$t$时模型的隐藏状态。$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$共同决定。<br>    3）$o^{(t)}$代表在序列索引号$t$时模型的输出。$o^{(t)}$只由模型当前的隐藏状态$h^{(t)}$决定。<br>    4）$L^{(t)}$代表在序列索引号$t$时模型的损失函数。<br>    5）$y^{(t)}$代表在序列索引号$t$时训练样本序列的真实输出。<br>    6）$U,W,V$这三个矩阵是我们的模型的线性关系参数，它在整个RNN网络中是共享的，这点和DNN很不相同。<br>    也正因为是共享了，它体现了RNN的模型的“循环反馈”的思想。　</p>
<h1 id="3-RNN前向传播算法"><a href="#3-RNN前向传播算法" class="headerlink" title="3.RNN前向传播算法"></a>3.RNN前向传播算法</h1><h1 id="4-RNN反向传播算法推导"><a href="#4-RNN反向传播算法推导" class="headerlink" title="4.RNN反向传播算法推导"></a>4.RNN反向传播算法推导</h1><h1 id="5-RNN小结"><a href="#5-RNN小结" class="headerlink" title="5.RNN小结"></a>5.RNN小结</h1><pre><code>上面调整参数的细节还没有了解之外，其他任务也算是了解得七七八八了吧，至于上面欠缺的图2，图3，我想等我真的学到的时候再进行添加吧。
RNN虽然理论上可以很好德解决序列数据的训练，但是它也像DNN一样有梯度小时时的问题，当序列很长的时候问题尤其严重。因此，上面的RNN模型一般不能直接用于应用领域。手写识别以及机器翻译等NLP领域实际应用比较广泛的事基于RNN模型的一个特例LSTM，接下来，将做LSTM的记录。由于目前LSTM的tensorflow版已经记录完成了，接下来的工作就是记录好LSTM的技术理论和流程。
</code></pre><p>ps：内容基本上都是抄的网络上和书上的，自己总结的东西还是少，只能说任重而道远了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/14/RNN/" data-id="cjifm66po00006onwe06vst6r" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习，待完善/">深度学习，待完善</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-perceptron" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/07/perceptron/" class="article-date">
  <time datetime="2018-06-07T03:00:57.000Z" itemprop="datePublished">2018-06-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/07/perceptron/">perceptron 感知机</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<h1 id="题记"><a href="#题记" class="headerlink" title="题记"></a>题记</h1><p>  最近终于能有一些时间空下来来整理一下这学期学到的东西了， 以后每天都会坚持写一写，就算是以后工作了，也会坚持写（我是这么立下flag的）。<br>  目前最需要整理的就是深度学习的东西，有一些深入的内容后面再学的时候会慢慢更新博客（好吧，由于也没人看，我自己跟自己说话，说得这么起劲）。<br>  关于深度学习的内容，基本上没一个内容是写得好的，然而既然我已经厚颜无耻地要写博客了，那我就尽力写一写，假装自己能写得不错，然后再洋洋得意一番。<br>  由于我是用tensorflow实践的深度学习，图也基本上是照着网上扒拉下来的，所以如果有侵权警告的话，请github给我留言。<br>  废话不说了，开始今天的内容吧。</p>
<h1 id="什么是感知机？"><a href="#什么是感知机？" class="headerlink" title="什么是感知机？"></a>什么是感知机？</h1><pre><code>这个题目怕是已经被写烂了，毕竟是整个体系里面相对来说比较简单的，同时也是入门模型。
</code></pre><p>  在介绍感知机之前，首先介绍一下二分类。<br><img src="https://iajqs.github.io/img/LSTM3-chain.png" alt=""><br></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/07/perceptron/" data-id="cjifm66r000036onwhx7kgpeq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-以太坊Dapp开发环境配置与实践" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/06/以太坊Dapp开发环境配置与实践/" class="article-date">
  <time datetime="2018-05-06T03:34:55.000Z" itemprop="datePublished">2018-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/06/以太坊Dapp开发环境配置与实践/">以太坊Dapp开发环境配置与实践</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、工具"><a href="#一、工具" class="headerlink" title="一、工具"></a>一、工具</h1><h2 id="需要安装的工具列表"><a href="#需要安装的工具列表" class="headerlink" title="需要安装的工具列表"></a>需要安装的工具列表</h2><p>如果你不喜欢把时间浪费在搭建开发环境上，可以使用汇智网的在线练习环境及教程：以太坊 DApp 开发实战入门–<a href="http://xc.hubwiz.com/course/5a952991adb3847553d205d1?affid=v2ex7878" target="_blank" rel="noopener">http://xc.hubwiz.com/course/5a952991adb3847553d205d1?affid=v2ex7878</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cnpm</span><br><span class="line">Node.js</span><br><span class="line">Geth</span><br><span class="line">solc</span><br><span class="line">web3</span><br><span class="line">truffle</span><br><span class="line">webpack</span><br></pre></td></tr></table></figure></p>
<h2 id="安装cnpm"><a href="#安装cnpm" class="headerlink" title="安装cnpm"></a>安装cnpm</h2><p>使用淘宝镜像安装， 速度会比较快<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure></p>
<p>安装完成后创建一个软链接<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; sudo ln -s /home/ubuntu/node-v4.5.0-linux-x86/bin/cnpm /usr/<span class="built_in">local</span>/bin/</span><br></pre></td></tr></table></figure></p>
<p>这样就可以在全局使用cnpm了</p>
<p>查看<code>cnpm</code>版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cnpm -v</span><br></pre></td></tr></table></figure></p>
<h2 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h2><p>直接登录官网下载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">官网地址：http://nodejs.cn/</span><br></pre></td></tr></table></figure></p>
<p>下载后直接安装即可。安装完毕，打开控制台窗口，查看nodejs版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; node -v</span><br></pre></td></tr></table></figure></p>
<h2 id="安装Geth"><a href="#安装Geth" class="headerlink" title="安装Geth"></a>安装Geth</h2><p>登录官网下载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下载地址：https://geth.ethereum.org/downloads/</span><br></pre></td></tr></table></figure></p>
<p>下载安装后，查看geth版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; geth version</span><br></pre></td></tr></table></figure></p>
<h2 id="安装solidity编译器"><a href="#安装solidity编译器" class="headerlink" title="安装solidity编译器"></a>安装solidity编译器</h2><p>使用cnpm安装，npm是真的强大，可以直接安装好多东西<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; npm install -g solc</span><br></pre></td></tr></table></figure></p>
<p>安装完毕后， 执行命令验证安装成功<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; solcjs -version</span><br></pre></td></tr></table></figure></p>
<p>执行出错，没有解决。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; Must provide a file</span><br></pre></td></tr></table></figure></p>
<h2 id="安装web3"><a href="#安装web3" class="headerlink" title="安装web3"></a>安装web3</h2><p>由于我使用的操作系统是git，需要先安装windows版的git命令行。<br>在<code>git bash</code>控制台输入以下命令(在这里卡了好久，一直以为是再cmd里输入一下命令)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cnpm install -g web3@0.20.2</span><br></pre></td></tr></table></figure></p>
<p>如果直接使用npm进行安装，会报错，可能是需要翻墙的关系。</p>
<h2 id="安装truffle"><a href="#安装truffle" class="headerlink" title="安装truffle"></a>安装truffle</h2><p>使用cnpm安装truffle开发框架<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cnpm install -g truffle</span><br></pre></td></tr></table></figure></p>
<p>查看truffle版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; truffle.cmd version</span><br></pre></td></tr></table></figure></p>
<p>windows系统下， 使用truffle要加上.cmd后缀，即使用windows的内核脚本文件，具体原因有待探究</p>
<h2 id="安装webpack"><a href="#安装webpack" class="headerlink" title="安装webpack"></a>安装webpack</h2><p>使用cnpm安装webpack<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; cnpm install -g webpack@3.11.0</span><br></pre></td></tr></table></figure></p>
<p>查看webpack版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; webpack -v</span><br></pre></td></tr></table></figure></p>
<h1 id="二、运行私链节点"><a href="#二、运行私链节点" class="headerlink" title="二、运行私链节点"></a>二、运行私链节点</h1><h2 id="2-1-创世块配置"><a href="#2-1-创世块配置" class="headerlink" title="2.1 创世块配置"></a>2.1 创世块配置</h2><p>创建一个文件夹，在其中创建私链的创世块配置文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; mkdir node1</span><br><span class="line">&gt; <span class="built_in">cd</span> node1</span><br><span class="line">\node1&gt; notepad private.json</span><br></pre></td></tr></table></figure></p>
<p>然后编辑内容如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"config"</span>:&#123;</span><br><span class="line">        <span class="attr">"chainId"</span>:<span class="number">7878</span>,</span><br><span class="line">        <span class="attr">"homesteadBlock"</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="attr">"eip155Block"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"eip158Block"</span>:<span class="number">0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    "difficulty":"200",     //挖矿难度，值越小难度越小</span><br><span class="line">    "gasLimit":"0xffffffff", //油费上限值，由于我们是私链，所以上限值设置为最大</span><br><span class="line">    "alloc":&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>config.chainId</code>：用来声明以太坊网络编号，选择一个大于10的数字即可<br><code>difficulty</code>：用来声明挖矿难度，越小的值难度越低，也就能更快速地出块<br><code>gasLimit</code>：油费上限值，在区块链上进行交易，需要消耗gas，如果需要消耗的gas超过了gasLimit，区块链将退回交易请求，不允执行</p>
<h2 id="初始化私链节点"><a href="#初始化私链节点" class="headerlink" title="初始化私链节点"></a>初始化私链节点</h2><p>执行geth的init命令初始化私链节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\node1&gt; geth --datadir .\data init private.json</span><br></pre></td></tr></table></figure></p>
<p>这会在node1目录下创建data目录，用来保存区块数据及账户信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\node1&gt; dir</span><br><span class="line">data private.json</span><br></pre></td></tr></table></figure></p>
<p>将上述命令写入一个脚本init.cmd里，便于下次初始化私链节点<br>内容如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">geth --data .\data init private.json</span><br></pre></td></tr></table></figure></p>
<h2 id="2-3-启动私链节点"><a href="#2-3-启动私链节点" class="headerlink" title="2.3 启动私链节点"></a>2.3 启动私链节点</h2><p>从指定的私链数据目录启动并设定一个不同的网络编号来启动节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\node1&gt; geth --rpc --rpcaddr 0.0.0.0 --rpccorsdomain <span class="string">"*"</span> --datadir .\data --networdid 7878 console</span><br></pre></td></tr></table></figure></p>
<p>这个也同样可以使用一个脚本console.cmd来保存以上的命令，便于下次启动私链节点</p>
<p>以后启动节点，只要直接执行这个脚本即可<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\node1&gt; console.cmd</span><br></pre></td></tr></table></figure></p>
<h2 id="2-4-账户管理"><a href="#2-4-账户管理" class="headerlink" title="2.4 账户管理"></a>2.4 账户管理</h2><h4 id="2-4-1查看账户列表"><a href="#2-4-1查看账户列表" class="headerlink" title="2.4.1查看账户列表"></a>2.4.1查看账户列表</h4><p>首先， 执行<code>console.cmd</code>进入geth控制台，使用eth对象的accounts属性查看目前的账户列表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; eth.accounts</span><br><span class="line">[]</span><br></pre></td></tr></table></figure></p>
<p>这个时候列表是空的， 我们需要创建一个用户</p>
<h4 id="2-4-2创建新账户"><a href="#2-4-2创建新账户" class="headerlink" title="2.4.2创建新账户"></a>2.4.2创建新账户</h4><p>在geth控制台， 使用personal对象的newAccount()方法创建一个新账户，参数为密码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; personal.newAccount(<span class="string">'123456'</span>)</span><br><span class="line"><span class="string">"0x79a1c7391c620882cebaffaf7b86e0c8236679a5"</span></span><br></pre></td></tr></table></figure></p>
<p>输出就是新创建的账户地址（公钥），你的输出不会和上面的示例相同。geth会保存到数据目录下的keystore文件中。</p>
<h4 id="2-4-3查询账户余额"><a href="#2-4-3查询账户余额" class="headerlink" title="2.4.3查询账户余额"></a>2.4.3查询账户余额</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; eth.getBalance(eth.accounts[0])</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<p>新创建的账户， 余额为0</p>
<h4 id="2-4-4挖矿"><a href="#2-4-4挖矿" class="headerlink" title="2.4.4挖矿"></a>2.4.4挖矿</h4><p>在geth控制台， 执行miner对象的start()方法来启动挖矿<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; miner.start(1)</span><br></pre></td></tr></table></figure></p>
<p>其中<code>start(1)</code>中的<code>1</code>表示开一个线程进行挖矿，如果不配置，就会让CPU全速运行，影响计算机的使用。</p>
<ul>
<li>运行一会后，主账号就会获取很多以太币，这个时候屏幕会快速刷屏，不用管，输入命令miner.stop()停止挖矿。</li>
<li>这里有个需要注意的地方，如果是第一次挖矿，需要等待加载到100才会开始挖矿，请耐心等待<br>看到挖出一些块之后，检查账户余额：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; eth.getBalance(eth.accounts[0])</span><br><span class="line">225000000000000000000</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>停止挖矿<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; miner.stop()</span><br></pre></td></tr></table></figure></p>
<h4 id="2-4-5解锁账户"><a href="#2-4-5解锁账户" class="headerlink" title="2.4.5解锁账户"></a>2.4.5解锁账户</h4><p>在部署合约是需要一个解锁的账户。<br>在geth控制台使用personal对象的unlockAccount()方法来解锁指定的账户，参数为账户地址和账户密码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; eth.unlockAccount(eth.accounts[0],<span class="string">"123456"</span>)</span><br></pre></td></tr></table></figure></p>
<p>三、构建示例项目</p>
<h4 id="3-1新建DApp项目"><a href="#3-1新建DApp项目" class="headerlink" title="3.1新建DApp项目"></a>3.1新建DApp项目</h4><p>执行一下命令创建项目目录并进入该目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; mkdir demo</span><br><span class="line">&gt; <span class="built_in">cd</span> demo</span><br></pre></td></tr></table></figure></p>
<p>接着使用truffle初始化项目框架结构<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; truffle.cmd unbox webpack</span><br></pre></td></tr></table></figure></p>
<p>很多教程是用<code>truffle init</code>,但是生成的项目会有一些问题，所以建议使用以上命令</p>
<h4 id="3-2安装项目依赖的NPM包"><a href="#3-2安装项目依赖的NPM包" class="headerlink" title="3.2安装项目依赖的NPM包"></a>3.2安装项目依赖的NPM包</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; npm install</span><br></pre></td></tr></table></figure>
<h4 id="3-3修改truffle配置"><a href="#3-3修改truffle配置" class="headerlink" title="3.3修改truffle配置"></a>3.3修改truffle配置</h4><p>打开truffle.js配置文件，内容如下：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Allows us to use ES6 in our migrations and tests.</span></span><br><span class="line"><span class="built_in">require</span>(<span class="string">'babel-register'</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  networks: &#123;</span><br><span class="line">    development: &#123;</span><br><span class="line">      host: <span class="string">'127.0.0.1'</span>,</span><br><span class="line">      port: <span class="number">8545</span>,</span><br><span class="line">      network_id: <span class="string">'*'</span> <span class="comment">// Match any network id</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>其中<code>host:127.0.0.1</code>等同于<code>host:localhost</code></li>
<li>端口号<code>port</code>需要根据私链所在的端口号来设置， 如果没有特别制定，geth默认端口号为8545</li>
</ul>
<h4 id="3-4启动节点"><a href="#3-4启动节点" class="headerlink" title="3.4启动节点"></a>3.4启动节点</h4><p>部署合约之前，我们先启动节点，直接执行我们上面保存的<code>console.cmd</code></p>
<h4 id="3-5编译合约"><a href="#3-5编译合约" class="headerlink" title="3.5编译合约"></a>3.5编译合约</h4><p>执行一下命令编译项目合约<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; truffle.cmd compile</span><br></pre></td></tr></table></figure></p>
<h4 id="3-6部署合约"><a href="#3-6部署合约" class="headerlink" title="3.6部署合约"></a>3.6部署合约</h4><p>执行一下命令来部署合约：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; truffle.cmd migrate</span><br></pre></td></tr></table></figure></p>
<p>执行部署合约之前要记得解锁账户，不然会报错。错误信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: authentication needed: password or unlock</span><br></pre></td></tr></table></figure></p>
<p>如果已经正确地解锁了账户，你会看到部署过程停止在如下状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Replacing Migrations...</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>这是因为部署合约其实就是一个交易，只要是交易就需要记录到区块链中，这个时候就需要挖矿来记录交易信息。</p>
<p>返回geth终端窗口，查看交易池的状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; txpool.status</span><br><span class="line">&#123;</span><br><span class="line">    pending:1,</span><br><span class="line">    queued:0 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>显示有一个挂起的交易，开始挖矿：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; miner.start(1)</span><br></pre></td></tr></table></figure></p>
<p>挖出一定区块后，再查看交易池的状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; txpool.status</span><br><span class="line">&#123;</span><br><span class="line">    pending:0,</span><br><span class="line">    queued:0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>交易提交完成，停止挖矿<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; miner.stop()</span><br></pre></td></tr></table></figure></p>
<h4 id="3-7启动DApp"><a href="#3-7启动DApp" class="headerlink" title="3.7启动DApp"></a>3.7启动DApp</h4><p>一开始，我看的教程是使用一下命令来启动<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; truffle deploy</span><br><span class="line">\demo&gt; truffle serve</span><br></pre></td></tr></table></figure></p>
<p>但是，这样会报这样的错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Serving static assets <span class="keyword">in</span> .\build on port 8080...</span><br><span class="line">TypeError: fsevents is not a constructor</span><br></pre></td></tr></table></figure></p>
<p>一些文章的解释说，是因为<code>truffle</code>的版本太高，建议删除并安装<a href="mailto:`truffle@3.2.1" target="_blank" rel="noopener">`truffle@3.2.1</a><code>，执行制定的操作后，发现安装不上</code><a href="mailto:truffle@3.2.1" target="_blank" rel="noopener">truffle@3.2.1</a>`，宣告失败，最后找到一个别的方法启动，代码如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\demo&gt; npm run dev</span><br></pre></td></tr></table></figure></p>
<p>在浏览器里访问<a href="http://localhost:8081" target="_blank" rel="noopener">http://localhost:8081</a> 即可，我的端口号是8081，汇智网教程的是8080，以你输入<code>npm run dev</code>之后控制台打印的网址为准。<br><img src="https://iajqs.github.io/img/MetaCoin界面.jpg" alt=""><br>在最后的页面显示中，还是会报错，错误信息如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;There was an error fetching your accounts.&quot;</span><br></pre></td></tr></table></figure></p>
<p>打开控制台，发现显示无法链接<code>127.0.0.1:9545</code>，打开项目文件夹下的app\javascripts，打开app.js，找到以下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.web3 = <span class="keyword">new</span> Web3(<span class="keyword">new</span> Web3.providers.HttpProvider(<span class="string">"http://127.0.0.1:9545"</span>));</span><br></pre></td></tr></table></figure></p>
<p>改为<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.web3 = <span class="keyword">new</span> Web3(<span class="keyword">new</span> Web3.providers.HttpProvider(<span class="string">"http://localhost:8545"</span>));</span><br></pre></td></tr></table></figure></p>
<p>刷新网页，发现还有错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: The method net_version does not exist/is not available</span><br></pre></td></tr></table></figure></p>
<p>暂时没有解决，学业不精，内牛满面。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/05/06/以太坊Dapp开发环境配置与实践/" data-id="cjifm66r400056onwic9wejdi" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/区块链/">区块链</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-plan" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/03/plan/" class="article-date">
  <time datetime="2018-05-03T01:19:28.000Z" itemprop="datePublished">2018-05-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/03/plan/">记录</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="需完善博客"><a href="#需完善博客" class="headerlink" title="需完善博客"></a>需完善博客</h1><ol>
<li><a href="../../02/LSTM/">LSTM</a></li>
</ol>
<h1 id="blockChain"><a href="#blockChain" class="headerlink" title="blockChain"></a>blockChain</h1><ul>
<li>打印以太零白皮书</li>
<li>完成以太零部署智能合约（发布token）</li>
<li>完成token发布后， 撰写整个发布流程</li>
</ul>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><ol>
<li>回实验室记录idea</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/05/03/plan/" data-id="cjifm66r200046onwlf5i3s20" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-LSTM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/02/LSTM/" class="article-date">
  <time datetime="2018-05-02T11:27:22.000Z" itemprop="datePublished">2018-05-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/02/LSTM/">LSTM-tensorflow</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="LSTM简介（抄自《tensorflow实战》）"><a href="#LSTM简介（抄自《tensorflow实战》）" class="headerlink" title="LSTM简介（抄自《tensorflow实战》）"></a>LSTM简介（抄自《tensorflow实战》）</h1><p>LSTM是Schmidhuber教授于1997年提出，它天生就是为了解决长程依赖而设计的，不需要特别复杂地调试超参数，默认就可以记住长期的信息。LSTM的内部结构相比RNN更复杂，如图1-1所示，其中包含了4层神经网络，其中小圆圈是point-wise的操作，比如向量加法、点乘等，而小矩形则代表一层可以学习参数的神经网络。LSTM单元上面的那条直线代表了LSTM的状态state，它会贯穿所有串联在一起的LSTM单元，从第一个LSTM单元一直流向最后一个LSTM单元，其中只有少量的线性干预和改变。状态state在这条隧道中传递时，LSTM单元可以对其添加或删减信息，这些对信息流的修改操作由LSTM中的Gates控制。这些Gates中包含了一个Sigmoid层和一个向量点乘的操作，这个Sigmoid层的输出是0到1之间的值（这个sigmoid的长相还挺奇怪呀），它直接控制了信息传递的比例。如果为0代表不允许信息传递，为1则代表让信息全部通过。每个LSTM单元中包含了3个这样的Gates，用来维护和控制单元的状态信息。凭借对状态信息的储存和修改，LSTM单元就可以实现长程记忆<br><img src="https://iajqs.github.io/img/LSTM3-chain.png" alt=""><br></p>
<p><center>图1 LSTM结构示意图</center><br><img src="https://iajqs.github.io/img/LSTM2-notation.png" alt=""><br></p>
<h1 id="LSTM的tensorflow实现和注解"><a href="#LSTM的tensorflow实现和注解" class="headerlink" title="LSTM的tensorflow实现和注解"></a>LSTM的tensorflow实现和注解</h1><p>下面只讲解主要的部分，其他内容请查看《tensorflow实战》</p>
<h4 id="1-导入数据："><a href="#1-导入数据：" class="headerlink" title="1.导入数据："></a>1.导入数据：</h4><ul>
<li>首先下载<code>PTB</code>数据并解压<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz</span><br><span class="line">tar xvf simple-examples.tgz</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>确保解压后的文件路径和接下来Python的执行路径一致。</p>
<ul>
<li><p>下载<code>TensorFlow Models</code>库，并进入目录<code>models/tutorials/rnn/ptb</code>，将其中的<code>reader.py</code>复制到库中<code>(site-packages)</code>，借助它读取数据内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/tensorflow/models.git</span><br><span class="line"><span class="built_in">cd</span> models/tutorials/rnn/ptb</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后导入<code>reader</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> reader</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-创建模型"><a href="#2-创建模型" class="headerlink" title="2.创建模型"></a>2.创建模型</h4><h4 id="构建LSTM单元"><a href="#构建LSTM单元" class="headerlink" title="构建LSTM单元"></a>构建LSTM单元</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">### size = hidden_size</span></span><br><span class="line">    <span class="comment">### state_is_tuple mean 2-tuple 也就是二元</span></span><br><span class="line">    <span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</span><br><span class="line">        size, forget_bias = <span class="number">0.0</span>, state_is_tuple = <span class="keyword">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment">## set the lstm_cell function to attn_cell</span></span><br><span class="line">attn_cell = lstm_cell</span><br><span class="line"><span class="comment">## if training and the config.keep_prob &lt; 1</span></span><br><span class="line"><span class="comment">### then add the Dropout layer to the lstm_cell</span></span><br><span class="line"><span class="keyword">if</span> is_training <span class="keyword">and</span> config.keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">attn_cell</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="comment">#### let the lstm_cell()'s output as the input for DropoutWrapper</span></span><br><span class="line">        <span class="keyword">return</span> tf.contrib.rnn.DropoutWrapper(</span><br><span class="line">            lstm_cell(), output_keep_prob = config.keep_prob</span><br><span class="line">        )</span><br><span class="line"><span class="comment">## pile the lstm_cell (i think here should be attn_cell) num_layers times</span></span><br><span class="line">cell = tf.contrib.rnn.MultiRNNCell(</span><br><span class="line">    [attn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(config.num_layers)],</span><br><span class="line">    state_is_tuple = <span class="keyword">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self._initial_state = cell.zero_state(batch_size, tf.float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>这里需要注意，<code>LSTM单元</code>可以读入一个单词并结合之前储存的状态<code>state</code>计算下一个单词出现的概率分布，并且每次读取一个单词后它的状态<code>state</code>会被更新。</li>
<li><font color="#FF0000">这里我并不是很理解使用<code>rnn.MultiRNNCell</code>将前面的<code>lstm_cell</code>多层堆叠到<code>cell</code>具体是什么意思，难道是说两个<code>LSTM单元</code>连接在一起（参考图1-1）吗？暂时只能这么理解了。</font></li>
<li><p><font color="#FF0000">并不理解<code>tf.contrib.rnn.DropoutWrapper</code>的具体作用,难道是所谓的：</font></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">状态state在这条隧道中传递时，LSTM单元可以对其添加或删减信息，这些对信息流的修改操作由LSTM中的Gates控制。</span><br></pre></td></tr></table></figure>
<ul>
<li>然而我依旧不能下定论</li>
</ul>
</li>
</ul>
<h4 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h4><font color="#FF0000">测试</font><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## limit the caculate operation in cpu</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/cpu:0"</span>):</span><br><span class="line">    <span class="comment">### embedding the data</span></span><br><span class="line">    embedding = tf.get_variable(</span><br><span class="line">        <span class="string">"embedding"</span>, [vocab_size, size], dtype=tf.float32</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">### get the embed of input_.input_data from embedding</span></span><br><span class="line">    <span class="comment">#### inputs: inputs[batch_size, word_size, vector_size]</span></span><br><span class="line">    &lt;!-- 晚上回去测试一下 --&gt;</span><br><span class="line">    <span class="comment">#### batch_size = config.batch_size</span></span><br><span class="line">    <span class="comment">#### word_size = config.num_steps</span></span><br><span class="line">    <span class="comment">#### vector_size = config.vector_size</span></span><br><span class="line">    inputs = tf.nn.embedding_lookup(embedding, input_.input_data)</span><br></pre></td></tr></table></figure><br><br><em> 注这里果然还是需要测试一下<code>input_.input_data</code>的数据是什么样的，我怀疑<code>PTBinput</code>返回的是想<code>Word2Vec</code>那样的<code>top</code>编号<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## if is_training and keep_prob &lt; 1, add the dropout</span></span><br><span class="line"><span class="comment">### the dropout(inputs, keep_prob) is mean to get (keep_prob / 1) x 100% data from input</span></span><br><span class="line"><span class="keyword">if</span> is_training <span class="keyword">and</span> config.keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">    inputs = tf.nn.dropout(inputs, config.keep_prob)</span><br></pre></td></tr></table></figure>

</em> 这里发现， 对输入数据也进行了<code>dropout</code>操作，得到<code>新的inputs</code>，后面的操作中， 将会把这个<code>新的inputs</code>输入给<code>cell</code>,那岂不是进行了两次<code>dropout</code><br><br>#### 使用模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">outputs = []</span><br><span class="line">state = self._initial_state</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"RNN"</span>):</span><br><span class="line">    <span class="comment">### set the loop size is num_steps</span></span><br><span class="line">    <span class="keyword">for</span> time_step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        <span class="comment">### at the sencond loop , use the tf.get_variable_scope().reuse_variables() set the reuse variables</span></span><br><span class="line">        <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">            tf.get_variable_scope().reuse_variables()</span><br><span class="line">        <span class="comment">### input the (inputs, state) to the LSTM cell</span></span><br><span class="line">        <span class="comment">### get the result(cell_output) and the updated state</span></span><br><span class="line">        <span class="comment">#### input[a, b, c]</span></span><br><span class="line">            <span class="comment">##### a: the index of sample in one batch</span></span><br><span class="line">            <span class="comment">##### b: the index of word in one sample</span></span><br><span class="line">            <span class="comment">##### c: the dimension number of the word's vector</span></span><br><span class="line">        (cell_output, state) = cell(inputs[:, time_step, :], state)</span><br><span class="line">        <span class="comment">### add the result to the output list</span></span><br><span class="line">        outputs.append(cell_output)</span><br></pre></td></tr></table></figure><br><br><em> <code>num_step</code> 反向传播时可以展开的步数
</em> <code>(cell_output, state) = cell(inputs[:, time_step, :], state)</code> 其中 <code>input[a, b, c]</code> 的a表示batch中的第几个样本，b表示样本中的第几个单词，c表示单词的向量表达式的维度，<code>inputs[:, time_step, :]</code>而这里表示的是所有样本的第time_step个单词。<font color="#FF0000">（这个输入好特别， 又是一个需要研究一下的点）</font>

<h4 id="设置模型配置信息"><a href="#设置模型配置信息" class="headerlink" title="设置模型配置信息"></a>设置模型配置信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># init the SamllConfig, ok, this is according with the modelize programing</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SmallConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">## init the weigth of the net</span></span><br><span class="line">    init_scale = <span class="number">0.1</span></span><br><span class="line">    <span class="comment">## set the learning rate</span></span><br><span class="line">    learning_rate = <span class="number">1.0</span></span><br><span class="line">    <span class="comment">## set the gradient's max norm</span></span><br><span class="line">    max_grad_norm = <span class="number">5</span></span><br><span class="line">    <span class="comment">## set the Stackingable layers</span></span><br><span class="line">    num_layers = <span class="number">2</span></span><br><span class="line">    <span class="comment">## LSTM gradient backpropagation's step</span></span><br><span class="line">    num_steps = <span class="number">20</span></span><br><span class="line">    <span class="comment">## the number of hidden node</span></span><br><span class="line">    hidden_size = <span class="number">200</span></span><br><span class="line">    <span class="comment">## initial learning rate could train times</span></span><br><span class="line">    max_epoch = <span class="number">4</span></span><br><span class="line">    <span class="comment">## altogether cound train times</span></span><br><span class="line">    max_max_epoch = <span class="number">13</span></span><br><span class="line">    <span class="comment">## dropout layer retain proportion</span></span><br><span class="line">    keep_prob = <span class="number">1.0</span></span><br><span class="line">    <span class="comment">## this is the learning rate's decay speed</span></span><br><span class="line">    lr_decay = <span class="number">0.5</span></span><br><span class="line">    <span class="comment">## batch_size is batch's size</span></span><br><span class="line">    batch_size = <span class="number">20</span></span><br><span class="line">    <span class="comment">## vocabulary's size</span></span><br><span class="line">    vocab_size = <span class="number">10000</span></span><br></pre></td></tr></table></figure>
<h4 id="3-训练"><a href="#3-训练" class="headerlink" title="3.训练"></a>3.训练</h4><ul>
<li>其中输入为session会话， </li>
<li>model模型</li>
<li>eval_op是否有测评操作<font color="#FF0000">（暂时不知道是为什么)</font></li>
<li><p>verbose是一个打印标记</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(session, model, eval_op = None, verbose = False)</span>:</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    costs = <span class="number">0.0</span></span><br><span class="line">    iters = <span class="number">0</span></span><br><span class="line">    state = session.run(model.initial_state)</span><br><span class="line">    <span class="comment">## create the fetches to get the run results</span></span><br><span class="line">    fetches = &#123;</span><br><span class="line">        <span class="string">"cost"</span>: model.cost,</span><br><span class="line">        <span class="string">"final_state"</span>: model.final_state</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> eval_op <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        fetches[<span class="string">"eval_op"</span>] = eval_op</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(model.input.epoch_size):</span><br><span class="line">        feed_dict = &#123;&#125;</span><br><span class="line">        <span class="comment">### get the LSTM's all state to feed_dict</span></span><br><span class="line">        <span class="comment">#### enumerate(["a", "b", "c"]) ==&gt; [(0, "a"), (1, "b"), (2, "c")]</span></span><br><span class="line">        <span class="comment">#### ok, now, I don's konw what is the c or h</span></span><br><span class="line">        <span class="keyword">for</span> i, (c, h) <span class="keyword">in</span> enumerate(model.initial_state):</span><br><span class="line">            feed_dict[c] = state[i].c</span><br><span class="line">            feed_dict[h] = state[i].h</span><br><span class="line"></span><br><span class="line">        <span class="comment">### run and get the result</span></span><br><span class="line">        vals = session.run(fetches, feed_dict)</span><br><span class="line">        cost = vals[<span class="string">"cost"</span>]</span><br><span class="line">        state = vals[<span class="string">"final_state"</span>]</span><br><span class="line"></span><br><span class="line">        costs += cost</span><br><span class="line">        iters += model.input.num_steps</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> verbose <span class="keyword">and</span> step % (model.input.epoch_size // <span class="number">10</span>) == <span class="number">10</span>:</span><br><span class="line">            print(<span class="string">"%.3f perplexity: %.3f speed: %.0f wps"</span> %</span><br><span class="line">                  (step * <span class="number">1.0</span>/model.input.epoch_size, np.exp(costs/iters),</span><br><span class="line">                   iters * model.input.batch_size / (time.time() - start_time)))</span><br><span class="line">    <span class="keyword">return</span> np.exp(costs / iters)</span><br></pre></td></tr></table></figure>
</li>
<li><font color="#FF0000"><code>feed_dict[c] = state[i].c</code> 和 <code>state[i].h</code>都不知道具体指的是什么</font></li>
<li>其他的还ok</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/05/02/LSTM/" data-id="cjifm66re000b6onwt8fl0n3l" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习，待完善/">深度学习，待完善</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Skip-Gram" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/30/Skip-Gram/" class="article-date">
  <time datetime="2018-04-30T07:24:22.887Z" itemprop="datePublished">2018-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>title: Skip-Gram-tensorflow<br>date: 2018-04-30 15:24:22</p>
<h2 id="tags-深度学习-待完善"><a href="#tags-深度学习-待完善" class="headerlink" title="tags: 深度学习 待完善"></a>tags: 深度学习 待完善</h2><h1 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h1><p>tensorflow<br>pycharm</p>
<h1 id="什么是Word2Vec和Embeddings？"><a href="#什么是Word2Vec和Embeddings？" class="headerlink" title="什么是Word2Vec和Embeddings？"></a>什么是Word2Vec和Embeddings？</h1><p>Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。</p>
<p>我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy这个词向量十分相近。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="../../../../img/Word2Vec.jpg" alt=""><br><br>Skip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。</p>
<p>Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。</p>
<h1 id="从tensorflow上得到需要注意的细节"><a href="#从tensorflow上得到需要注意的细节" class="headerlink" title="从tensorflow上得到需要注意的细节"></a>从tensorflow上得到需要注意的细节</h1><p>《tensorflow实战》的例程：<br>1.先统计训练文本中的所有单词的tf值<br>2.按照tf值进行排序，并用每个词在排序表中下标来表示这个词<br>3.使用embeding的方式将下标转换成一个128为的随机矩阵， 然后后面会通过计算损失函数自动更新这个矩阵， 这个embeding矩阵就代表着这个词<br>4.最后会得到一个训练好的embeding表<br>5.可以通过计算某一个词的下标在embeding表中找到与之相似度较高的词下标</p>
<hr>
<h1 id="根据和学长们的讨论，这里很有必要追加一些内容"><a href="#根据和学长们的讨论，这里很有必要追加一些内容" class="headerlink" title="根据和学长们的讨论，这里很有必要追加一些内容"></a>根据和学长们的讨论，这里很有必要追加一些内容</h1><h4 id="这个Fake-Task是训练什么？计算的损失是谁的损失？"><a href="#这个Fake-Task是训练什么？计算的损失是谁的损失？" class="headerlink" title="这个Fake Task是训练什么？计算的损失是谁的损失？"></a>这个Fake Task是训练什么？计算的损失是谁的损失？</h4><p>  前面我们已经知道，我们会将词转换成embeding的形式，然后交给模型去训练，最后我们会得到一个训练好的embeding表。<br>  从整体上来看，这个embeding是一个权重矩阵，它是某个词的特征码（这么说的原因是为了体现特征码接近的词，特征也比较接近）。<br>  而训练过程中调整的模型（skip-gram）的作用是，输入一个词向量，会输出一个与词向量同等规模的向量。<br>  这个向量不一定代表某个词，但可以与正确的输出结果计算损失，然后通过这个损失调整词向量和模型里的参数。emmm，从整体上来看，这个词向量其实也是参数之一。<br>  那么，说到这里，总结一下，Fake Task的真正意图是调整词向量，也就是所谓的权重矩阵，而这个模型本身具有的测试功能除了调整权重矩阵之外，没有别的用途（只就这里来讲）。计算的损失是skip-gram模型的损失，但这个模型后面不会再使用。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/30/Skip-Gram/" data-id="cjifm66qu00016onwpws76obl" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/区块链/">区块链</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习，待完善/">深度学习，待完善</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/区块链/" style="font-size: 10px;">区块链</a> <a href="/tags/深度学习/" style="font-size: 10px;">深度学习</a> <a href="/tags/深度学习，待完善/" style="font-size: 20px;">深度学习，待完善</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/06/14/RNN/">rnn</a>
          </li>
        
          <li>
            <a href="/2018/06/07/perceptron/">perceptron 感知机</a>
          </li>
        
          <li>
            <a href="/2018/05/06/以太坊Dapp开发环境配置与实践/">以太坊Dapp开发环境配置与实践</a>
          </li>
        
          <li>
            <a href="/2018/05/03/plan/">记录</a>
          </li>
        
          <li>
            <a href="/2018/05/02/LSTM/">LSTM-tensorflow</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>